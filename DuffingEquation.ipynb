{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "668a5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from HamiltonianNeuralNetwork.Train import *\n",
    "from Systems.DuffingSystem import *\n",
    "from Systems.Generate_Data import *\n",
    "from HamiltonianNeuralNetwork.pHNN import *\n",
    "from NumericalIntegration.Numerical_Integration import *\n",
    "\n",
    "torch.random.manual_seed(1)\n",
    "np.random.seed(33)\n",
    "#torch.set_default_device(\"mps\")\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['lines.markersize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "colors = sns.color_palette([(0.6,0.8,.8), (1,0.7,0.3), (0.2,0.7,0.2), (0.8,0,0.2), (0,0.4,1), (0.6,0.5,.9), (0.5,0.3,.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c52bbc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_condition():\n",
    "    def sampler():\n",
    "        u0 =  np.random.rand(2) * 2 - 1\n",
    "        radius = np.sqrt(np.random.uniform(0.5, 1.5))  \n",
    "        u0 /= np.sqrt((u0 ** 2).sum()) * (radius)\n",
    "        #u0 = np.random.rand(2)*0\n",
    "        return u0.flatten()\n",
    "    return sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "703200fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef generate_data(ntrajectories, t_sample,system,integrator, true_derivatives = False,H0=None,u0s=None,data_type = torch.float32):\\n    #Parameters\\n    nstates = system.nstates\\n    traj_length = t_sample.shape[0] \\n\\n    #Initializing \\n    u = np.zeros((ntrajectories,traj_length,nstates))\\n    dudt = np.zeros_like(u)\\n    t = np.zeros((ntrajectories,traj_length))\\n\\n    u0_ = np.zeros((ntrajectories,nstates))\\n\\n    for i in tqdm(range(ntrajectories)):\\n        if u0s is not None:\\n            u0 = np.array(u0s[i])\\n            u[i], dudt[i], t[i],u0_[i] = system.sample_trajectory(t=t_sample,u0=u0,integrator=integrator)\\n        else:\\n            u[i], dudt[i], t[i],u0_[i] = system.sample_trajectory(t=t_sample,integrator=integrator)\\n    \\n    #Reshaping\\n    dt = torch.tensor([t[0, 1] - t[0, 0]], dtype=data_type)\\n    u_start = torch.tensor(u[:, :-1], dtype=data_type).reshape(-1, nstates)\\n    u_end = torch.tensor(u[:, 1:], dtype=data_type).reshape(-1, nstates)\\n    t_start = torch.tensor(t[:, :-1], dtype=data_type).reshape(-1, 1)\\n    dt = dt * torch.ones_like(t_start, dtype=data_type)\\n\\n    if true_derivatives:\\n        dudt = torch.tensor(dudt[:, :-1], dtype=data_type).reshape(-1, 1, nstates)\\n    else:\\n        dudt = (u_end - u_start).clone().detach() / dt[0, 0]\\n\\n    u_exact = u\\n    return (u_start, u_end,t_start, dt), dudt, u_exact, u0_\\n\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "def generate_data(ntrajectories, t_sample,system,integrator, true_derivatives = False,H0=None,u0s=None,data_type = torch.float32):\n",
    "    #Parameters\n",
    "    nstates = system.nstates\n",
    "    traj_length = t_sample.shape[0] \n",
    "\n",
    "    #Initializing \n",
    "    u = np.zeros((ntrajectories,traj_length,nstates))\n",
    "    dudt = np.zeros_like(u)\n",
    "    t = np.zeros((ntrajectories,traj_length))\n",
    "\n",
    "    u0_ = np.zeros((ntrajectories,nstates))\n",
    "\n",
    "    for i in tqdm(range(ntrajectories)):\n",
    "        if u0s is not None:\n",
    "            u0 = np.array(u0s[i])\n",
    "            u[i], dudt[i], t[i],u0_[i] = system.sample_trajectory(t=t_sample,u0=u0,integrator=integrator)\n",
    "        else:\n",
    "            u[i], dudt[i], t[i],u0_[i] = system.sample_trajectory(t=t_sample,integrator=integrator)\n",
    "    \n",
    "    #Reshaping\n",
    "    dt = torch.tensor([t[0, 1] - t[0, 0]], dtype=data_type)\n",
    "    u_start = torch.tensor(u[:, :-1], dtype=data_type).reshape(-1, nstates)\n",
    "    u_end = torch.tensor(u[:, 1:], dtype=data_type).reshape(-1, nstates)\n",
    "    t_start = torch.tensor(t[:, :-1], dtype=data_type).reshape(-1, 1)\n",
    "    dt = dt * torch.ones_like(t_start, dtype=data_type)\n",
    "\n",
    "    if true_derivatives:\n",
    "        dudt = torch.tensor(dudt[:, :-1], dtype=data_type).reshape(-1, 1, nstates)\n",
    "    else:\n",
    "        dudt = (u_end - u_start).clone().detach() / dt[0, 0]\n",
    "\n",
    "    u_exact = u\n",
    "    return (u_start, u_end,t_start, dt), dudt, u_exact, u0_\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280a491",
   "metadata": {},
   "source": [
    "### Looking at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4dabb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max_train = 18000\n",
    "alpha, beta, omega, gamma, delta = -1, 1, 1.4, 0.39, 0.1\n",
    "dt_per_period = 100\n",
    "nsamples_train = 18000/(2 * np.pi / omega / dt_per_period)\n",
    "ntraj_train = 1\n",
    "\n",
    "dt_train = T_max_train/nsamples_train\n",
    "nt_train = round(T_max_train / dt_train)\n",
    "t_train = np.linspace(0, T_max_train, nt_train + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "14d078e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nintegrator = \"midpoint\"\\n\\nsys = DuffingSystem(alpha = alpha, beta = beta, omega = omega, delta = delta, gamma=gamma)\\n\\n(u_start, u_end, t_start, dt), dudt, u_train, u0s_train  =  generate_data(system=sys,ntrajectories =ntraj_train, t_sample = t_train,integrator=integrator)\\ntrain_data = (u_start, u_end,t_start, dt), dudt\\n\\npstep = 100\\nplt.scatter(u_train[0][::pstep,0],u_train[0][::pstep,1])\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "integrator = \"midpoint\"\n",
    "\n",
    "sys = DuffingSystem(alpha = alpha, beta = beta, omega = omega, delta = delta, gamma=gamma)\n",
    "\n",
    "(u_start, u_end, t_start, dt), dudt, u_train, u0s_train  =  generate_data(system=sys,ntrajectories =ntraj_train, t_sample = t_train,integrator=integrator)\n",
    "train_data = (u_start, u_end,t_start, dt), dudt\n",
    "\n",
    "pstep = 100\n",
    "plt.scatter(u_train[0][::pstep,0],u_train[0][::pstep,1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0d5c3070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport torch\\nfrom tqdm import trange\\nimport datetime\\n\\n\\ndef Batch_Data(data,batch_size,shuffle):\\n    #Power of 2\\n \\n    nsamples = data[1].shape[0]\\n\\n    if shuffle:\\n        permutation = torch.randperm(nsamples)\\n    else:\\n        permutation = torch.arange(nsamples)\\n\\n    nbatches = np.ceil(nsamples/batch_size).astype(int)\\n    batched = [(None,None)] *nbatches  #((x_start, x_end, t_start, t_end, dt, u), dxdt)\\n\\n    for i in range(nbatches):\\n        indices = permutation[i * batch_size : (i + 1) * batch_size]\\n        input_tuple = [data[0][j][indices] for j in range(len(data[0]))]\\n        dudt = data[1][indices]\\n        batched[i] = (input_tuple, dudt)\\n\\n    return batched\\n\\n\\ndef penalty_loss(model,time, lam_F=1e-4,lam_N=1e-4):\\n    penalty = 0\\n    penalty += lam_F*torch.mean(torch.abs(model.External_Force(time.reshape(-1,1))))\\n    penalty += lam_N*torch.mean(torch.abs(model.Get_N()))\\n    return penalty\\n\\ndef train_one_epoch(model,batched_train_data,loss_func,optimizer,integrator):\\n    computed_loss = 0.0\\n    optimizer.zero_grad()\\n    for input_tuple, dudt in batched_train_data:\\n      \\n        (u_start, u_end, t_start, dt) = input_tuple\\n        n,m = u_start.shape\\n        #Reshaping\\n        if n ==1:\\n            u_start = u_start.view(-1)\\n\\n        dudt = dudt.view(n,m)\\n        #Estimating dudt\\n \\n\\n        dudt_est = model.time_derivative_step(integrator = integrator, u_start = u_start,t_start = t_start,u_end = u_end,dt = dt)\\n        loss = loss_func(dudt_est,dudt)\\n        loss += penalty_loss(model,t_start)\\n        loss.backward()\\n        optimizer.step()\\n        optimizer.zero_grad()\\n        computed_loss += loss.item()\\n\\n    return computed_loss / len(batched_train_data)\\n\\ndef compute_validation_loss(model, integrator, val_data, valdata_batched, loss_func):\\n    val_loss = 0\\n    if valdata_batched is not None:\\n        for input_tuple, dudt in valdata_batched:\\n            (u_start, u_end, t_start, dt) = input_tuple\\n            n,m = u_start.shape\\n            #Reshaping\\n            if n ==1:\\n                u_start = u_start.view(-1)\\n            #u_start = u_start.requires_grad_()\\n            dudt = dudt.view(n,m)\\n        \\n            dudt_est = model.time_derivative_step(integrator = integrator, u_start = u_start,t_start = t_start,u_end = u_end,dt = dt)\\n\\n            val_loss += loss_func(dudt_est, dudt).item()\\n            val_loss += penalty_loss(model,t_start).item()\\n    else:\\n        (u_start, u_end, t_start, dt), dudt = val_data\\n        n,m = u_start.shape\\n        #Reshaping\\n        if n ==1:\\n            u_start = u_start.view(-1)\\n        #u_start = u_start.requires_grad_()\\n        dudt = dudt.view(n,m)\\n        dudt_est = model.time_derivative(integrator, u_start,u_end,dt)\\n        val_loss = loss_func(dudt_est, dudt).item()\\n        val_loss += penalty_loss(model,t_start).item()\\n    val_loss = val_loss / len(valdata_batched)\\n    return val_loss#.item() #float(val_loss.detach().numpy())\\n    \\n\\ndef train(model,integrator, train_data,val_data, optimizer,shuffle,loss_func=torch.nn.MSELoss(),batch_size=1024,epochs = 20, verbose =True):\\n\\n   \\n    trainingdetails={}\\n    train_batch = Batch_Data(train_data, batch_size, shuffle)\\n    valdata_batched = Batch_Data(val_data, batch_size, False)\\n\\n    if optimizer is None:\\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\\n\\n    loss_list = []\\n    val_loss_list =  []\\n\\n\\n\\n    with trange(epochs) as steps:\\n        for epoch in steps:\\n            if shuffle:\\n                train_batch = Batch_Data(train_data,batch_size,shuffle)\\n            model.train(True) \\n            start = datetime.datetime.now() \\n            avg_loss = train_one_epoch(model,train_batch,loss_func,optimizer,integrator)\\n            end = datetime.datetime.now() \\n  \\n            loss_list.append(avg_loss)\\n            model.train(False) \\n            if verbose: #Print\\n                steps.set_postfix(epoch=epoch, loss=avg_loss)\\n\\n            if val_data is not None:\\n                start = datetime.datetime.now()\\n                vloss = compute_validation_loss(model, integrator, val_data, valdata_batched, loss_func)\\n                end = datetime.datetime.now()\\n               \\n                val_loss_list.append(vloss)\\n\\n            trainingdetails[\"epochs\"] = epoch + 1\\n            trainingdetails[\"val_loss\"] = vloss\\n            trainingdetails[\"train_loss\"] = avg_loss\\n\\n\\n\\n    print(val_loss_list)\\n    print(loss_list)\\n     # Plot the loss curve\\n    plt.figure(figsize=(7, 4))\\n    plt.plot(loss_list, label = \"Training Loss\")\\n    plt.plot(val_loss_list,label = \"Validation Loss\")\\n    plt.legend()\\n    plt.yscale(\\'log\\')\\n    plt.xlabel(\\'Epoch\\')\\n    plt.ylabel(\\'Loss\\')\\n    plt.title(\\'Training Loss\\')\\n    plt.show()\\n\\n    shape_data = (len(train_batch),train_batch[0][0][0].shape)\\n\\n    return model,trainingdetails\\n\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange\n",
    "import datetime\n",
    "\n",
    "\n",
    "def Batch_Data(data,batch_size,shuffle):\n",
    "    #Power of 2\n",
    " \n",
    "    nsamples = data[1].shape[0]\n",
    "\n",
    "    if shuffle:\n",
    "        permutation = torch.randperm(nsamples)\n",
    "    else:\n",
    "        permutation = torch.arange(nsamples)\n",
    "\n",
    "    nbatches = np.ceil(nsamples/batch_size).astype(int)\n",
    "    batched = [(None,None)] *nbatches  #((x_start, x_end, t_start, t_end, dt, u), dxdt)\n",
    "\n",
    "    for i in range(nbatches):\n",
    "        indices = permutation[i * batch_size : (i + 1) * batch_size]\n",
    "        input_tuple = [data[0][j][indices] for j in range(len(data[0]))]\n",
    "        dudt = data[1][indices]\n",
    "        batched[i] = (input_tuple, dudt)\n",
    "\n",
    "    return batched\n",
    "\n",
    "\n",
    "def penalty_loss(model,time, lam_F=1e-4,lam_N=1e-4):\n",
    "    penalty = 0\n",
    "    penalty += lam_F*torch.mean(torch.abs(model.External_Force(time.reshape(-1,1))))\n",
    "    penalty += lam_N*torch.mean(torch.abs(model.Get_N()))\n",
    "    return penalty\n",
    "\n",
    "def train_one_epoch(model,batched_train_data,loss_func,optimizer,integrator):\n",
    "    computed_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for input_tuple, dudt in batched_train_data:\n",
    "      \n",
    "        (u_start, u_end, t_start, dt) = input_tuple\n",
    "        n,m = u_start.shape\n",
    "        #Reshaping\n",
    "        if n ==1:\n",
    "            u_start = u_start.view(-1)\n",
    "\n",
    "        dudt = dudt.view(n,m)\n",
    "        #Estimating dudt\n",
    " \n",
    "\n",
    "        dudt_est = model.time_derivative_step(integrator = integrator, u_start = u_start,t_start = t_start,u_end = u_end,dt = dt)\n",
    "        loss = loss_func(dudt_est,dudt)\n",
    "        loss += penalty_loss(model,t_start)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        computed_loss += loss.item()\n",
    "\n",
    "    return computed_loss / len(batched_train_data)\n",
    "\n",
    "def compute_validation_loss(model, integrator, val_data, valdata_batched, loss_func):\n",
    "    val_loss = 0\n",
    "    if valdata_batched is not None:\n",
    "        for input_tuple, dudt in valdata_batched:\n",
    "            (u_start, u_end, t_start, dt) = input_tuple\n",
    "            n,m = u_start.shape\n",
    "            #Reshaping\n",
    "            if n ==1:\n",
    "                u_start = u_start.view(-1)\n",
    "            #u_start = u_start.requires_grad_()\n",
    "            dudt = dudt.view(n,m)\n",
    "        \n",
    "            dudt_est = model.time_derivative_step(integrator = integrator, u_start = u_start,t_start = t_start,u_end = u_end,dt = dt)\n",
    "\n",
    "            val_loss += loss_func(dudt_est, dudt).item()\n",
    "            val_loss += penalty_loss(model,t_start).item()\n",
    "    else:\n",
    "        (u_start, u_end, t_start, dt), dudt = val_data\n",
    "        n,m = u_start.shape\n",
    "        #Reshaping\n",
    "        if n ==1:\n",
    "            u_start = u_start.view(-1)\n",
    "        #u_start = u_start.requires_grad_()\n",
    "        dudt = dudt.view(n,m)\n",
    "        dudt_est = model.time_derivative(integrator, u_start,u_end,dt)\n",
    "        val_loss = loss_func(dudt_est, dudt).item()\n",
    "        val_loss += penalty_loss(model,t_start).item()\n",
    "    val_loss = val_loss / len(valdata_batched)\n",
    "    return val_loss#.item() #float(val_loss.detach().numpy())\n",
    "    \n",
    "\n",
    "def train(model,integrator, train_data,val_data, optimizer,shuffle,loss_func=torch.nn.MSELoss(),batch_size=1024,epochs = 20, verbose =True):\n",
    "\n",
    "   \n",
    "    trainingdetails={}\n",
    "    train_batch = Batch_Data(train_data, batch_size, shuffle)\n",
    "    valdata_batched = Batch_Data(val_data, batch_size, False)\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    loss_list = []\n",
    "    val_loss_list =  []\n",
    "\n",
    "\n",
    "\n",
    "    with trange(epochs) as steps:\n",
    "        for epoch in steps:\n",
    "            if shuffle:\n",
    "                train_batch = Batch_Data(train_data,batch_size,shuffle)\n",
    "            model.train(True) \n",
    "            start = datetime.datetime.now() \n",
    "            avg_loss = train_one_epoch(model,train_batch,loss_func,optimizer,integrator)\n",
    "            end = datetime.datetime.now() \n",
    "  \n",
    "            loss_list.append(avg_loss)\n",
    "            model.train(False) \n",
    "            if verbose: #Print\n",
    "                steps.set_postfix(epoch=epoch, loss=avg_loss)\n",
    "\n",
    "            if val_data is not None:\n",
    "                start = datetime.datetime.now()\n",
    "                vloss = compute_validation_loss(model, integrator, val_data, valdata_batched, loss_func)\n",
    "                end = datetime.datetime.now()\n",
    "               \n",
    "                val_loss_list.append(vloss)\n",
    "\n",
    "            trainingdetails[\"epochs\"] = epoch + 1\n",
    "            trainingdetails[\"val_loss\"] = vloss\n",
    "            trainingdetails[\"train_loss\"] = avg_loss\n",
    "\n",
    "\n",
    "\n",
    "    print(val_loss_list)\n",
    "    print(loss_list)\n",
    "     # Plot the loss curve\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(loss_list, label = \"Training Loss\")\n",
    "    plt.plot(val_loss_list,label = \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()\n",
    "\n",
    "    shape_data = (len(train_batch),train_batch[0][0][0].shape)\n",
    "\n",
    "    return model,trainingdetails\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "590085de",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max_train = 2*np.pi\n",
    "chaos = False\n",
    "if chaos == True:\n",
    "    alpha, beta, omega, gamma, delta = 1, 1, 1.4, 0.39, 0.1 \n",
    "else:\n",
    "    alpha, beta, omega, gamma, delta = -1, 1, 1.2, 0.2, 0.3\n",
    "nsamples_train = 100*T_max_train/2\n",
    "ntraj_train =1750#700# 70\n",
    "T_max_val = T_max_train\n",
    "nsamples_val = nsamples_train\n",
    "ntraj_val = 750#300#30\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "\n",
    "act_1 = PAU()\n",
    "act_2 = PAU()\n",
    "act_3 = nn.Softplus()\n",
    "\n",
    "\n",
    "\n",
    "sys = DuffingSystem(alpha = alpha, beta = beta, omega = omega, delta = delta, gamma=gamma)\n",
    "\n",
    "dt_train = T_max_train/nsamples_train\n",
    "nt_train = round(T_max_train / dt_train)\n",
    "t_train = np.linspace(0, T_max_train, nt_train + 1)\n",
    "\n",
    "dt_val = T_max_val/nsamples_val\n",
    "nt_val = round(T_max_val / dt_val)\n",
    "t_val= np.linspace(0, T_max_val, nt_val + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "10a1401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Duffing_H_loss_func(dudt,dudt_est):\n",
    "    func = torch.nn.MSELoss()\n",
    "    MSE_dudt = func(dudt_est,dudt)\n",
    "    return MSE_dudt\n",
    "\n",
    "\n",
    "def Duffing_penalty_loss_func(model,time, lam_F=1e-4,lam_N=1e-4):\n",
    "    penalty = 0\n",
    "    penalty += lam_F*torch.mean(torch.abs(model.External_Force(time.reshape(-1,1)).detach()))\n",
    "    penalty += lam_N*torch.mean(torch.abs(model.Get_N().detach()))\n",
    "    return penalty\n",
    "\"\"\"\n",
    "def Duffing_penalty_loss_func(model, time, lam_F=1e-4, lam_N=1e-4):\n",
    "    with torch.no_grad(): \n",
    "        F = model.External_Force(time.reshape(-1, 1)).detach()\n",
    "        N = model.Get_N().detach()\n",
    "    return lam_F * torch.mean(torch.abs(F)) + lam_N * torch.mean(torch.abs(N))\n",
    "\"\"\"\n",
    "loss_func = loss_wrapper(Duffing_H_loss_func)\n",
    "penalty_func = loss_wrapper(Duffing_penalty_loss_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6833dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1750/1750 [00:16<00:00, 105.81it/s]\n",
      "100%|██████████| 750/750 [00:06<00:00, 108.13it/s]\n"
     ]
    }
   ],
   "source": [
    "integrator = \"midpoint\"\n",
    "\n",
    "(u_start, u_end, t_start, dt), dudt, u_train, u0s_train  =  generate_data(system=sys,ntrajectories =ntraj_train, t_sample = t_train,integrator=integrator)\n",
    "train_data = (u_start, u_end,t_start, dt), dudt\n",
    "\n",
    "(u_start, u_end,t_start, dt), dudt, u_val, u0s_val =  generate_data(system=sys,ntrajectories =ntraj_val, t_sample = t_val,integrator=integrator)\n",
    "val_data = (u_start, u_end, t_start, dt), dudt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "69a49a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [3:08:58<00:00, 226.77s/it, epoch=49, loss=4.71e-5]  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Hamiltonian_est = BaseHamiltonianNeuralNetwork(nstates = 2, act_1 = act_1, act_2 = act_2, act_3 = act_3)\n",
    "External_Forces_est = ExternalForceNeuralNetwork(nstates = 2, act_1 = act_1, act_2 = act_2, act_3 = act_3)\n",
    "\n",
    "\n",
    "model_exp = PortHamiltonianNeuralNetwork(nstates = 2, S = sys.S, Hamiltonian_est = Hamiltonian_est, External_Forces_est = External_Forces_est)\n",
    "\n",
    "optimizer_exp = torch.optim.Adam(model_exp.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "trainer = Training(model = model_exp,integrator = integrator, train_data = train_data, val_data = val_data,optimizer = optimizer_exp, system = sys, batch_size=batch_size,epochs = epochs)\n",
    "model_exp, trainingdetails_symp = trainer.train(loss_func=loss_func,penalty_func = penalty_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "147ae814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 35.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_start = u[i : i + 1, :]:  tensor([[0.9308, 0.5149]])\n",
      "t_start = t_step:  tensor(0.)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5142, -0.0343], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9359, 0.5145], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9359, 0.5145]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0100)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5139, -0.0401], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9411, 0.5141], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9411, 0.5141]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0200)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5134, -0.0459], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9462, 0.5137], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9462, 0.5137]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0300)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5129, -0.0520], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9513, 0.5131], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9513, 0.5131]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0400)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5123, -0.0582], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9565, 0.5126], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9565, 0.5126]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0500)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5117, -0.0645], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9616, 0.5119], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9616, 0.5119]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0600)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5111, -0.0709], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9667, 0.5112], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9667, 0.5112]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0700)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5103, -0.0775], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9718, 0.5104], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9718, 0.5104]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0800)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5095, -0.0842], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9769, 0.5096], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9769, 0.5096]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0900)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5086, -0.0910], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9820, 0.5087], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9820, 0.5087]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1001)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5077, -0.0980], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9871, 0.5077], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9871, 0.5077]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1101)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5067, -0.1051], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9921, 0.5067], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9921, 0.5067]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1201)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5056, -0.1123], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9972, 0.5055], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9972, 0.5055]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1301)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5045, -0.1196], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0022, 0.5043], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0022, 0.5043]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1401)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5032, -0.1270], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0073, 0.5031], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0073, 0.5031]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1501)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5019, -0.1346], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0123, 0.5017], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0123, 0.5017]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1601)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.5006, -0.1422], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0173, 0.5003], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0173, 0.5003]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1701)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4991, -0.1499], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0223, 0.4988], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0223, 0.4988]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1801)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4976, -0.1578], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0273, 0.4972], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0273, 0.4972]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1901)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4960, -0.1657], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0322, 0.4956], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0322, 0.4956]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2001)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4943, -0.1738], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0372, 0.4938], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0372, 0.4938]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2101)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4925, -0.1819], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0421, 0.4920], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0421, 0.4920]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2201)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4907, -0.1902], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0470, 0.4901], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0470, 0.4901]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2301)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4887, -0.1985], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0519, 0.4881], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0519, 0.4881]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2401)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4867, -0.2069], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0568, 0.4860], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0568, 0.4860]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2501)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4846, -0.2154], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0616, 0.4839], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0616, 0.4839]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2601)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4824, -0.2240], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0664, 0.4816], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0664, 0.4816]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2701)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4801, -0.2327], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0713, 0.4793], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0713, 0.4793]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2801)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4778, -0.2415], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0760, 0.4769], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0760, 0.4769]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2901)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4753, -0.2503], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0808, 0.4744], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0808, 0.4744]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3002)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4728, -0.2592], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0855, 0.4718], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0855, 0.4718]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3102)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4702, -0.2681], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0902, 0.4691], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0902, 0.4691]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3202)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4674, -0.2771], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0949, 0.4663], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0949, 0.4663]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3302)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4646, -0.2862], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0995, 0.4635], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0995, 0.4635]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3402)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4617, -0.2953], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1042, 0.4605], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1042, 0.4605]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3502)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4587, -0.3045], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1088, 0.4575], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1088, 0.4575]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3602)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4557, -0.3136], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1133, 0.4543], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1133, 0.4543]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3702)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4525, -0.3229], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1178, 0.4511], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1178, 0.4511]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3802)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4492, -0.3321], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1223, 0.4478], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1223, 0.4478]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3902)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4459, -0.3414], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1268, 0.4444], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1268, 0.4444]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4002)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4424, -0.3507], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1312, 0.4409], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1312, 0.4409]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4102)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4389, -0.3601], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1356, 0.4373], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1356, 0.4373]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4202)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4352, -0.3694], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1400, 0.4336], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1400, 0.4336]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4302)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4315, -0.3788], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1443, 0.4298], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1443, 0.4298]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4402)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4277, -0.3881], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1486, 0.4259], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1486, 0.4259]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4502)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4237, -0.3975], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1528, 0.4219], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1528, 0.4219]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4602)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4197, -0.4068], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1570, 0.4178], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1570, 0.4178]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4702)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4156, -0.4162], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1612, 0.4137], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1612, 0.4137]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4802)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4114, -0.4255], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1653, 0.4094], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1653, 0.4094]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4902)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4071, -0.4347], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1694, 0.4051], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1694, 0.4051]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5003)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.4027, -0.4440], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1734, 0.4006], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1734, 0.4006]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5103)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3982, -0.4532], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1774, 0.3961], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1774, 0.3961]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5203)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3937, -0.4624], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1813, 0.3915], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1813, 0.3915]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5303)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3890, -0.4715], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1852, 0.3868], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1852, 0.3868]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5403)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3843, -0.4805], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1890, 0.3819], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1890, 0.3819]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5503)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3794, -0.4895], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1928, 0.3770], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1928, 0.3770]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5603)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3745, -0.4985], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1966, 0.3721], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1966, 0.3721]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5703)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3695, -0.5073], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2003, 0.3670], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2003, 0.3670]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5803)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3645, -0.5160], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2039, 0.3618], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2039, 0.3618]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5903)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3593, -0.5247], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2075, 0.3566], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2075, 0.3566]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6003)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3540, -0.5333], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2111, 0.3512], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2111, 0.3512]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6103)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3486, -0.5418], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2146, 0.3458], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2146, 0.3458]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6203)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3431, -0.5502], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2180, 0.3403], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2180, 0.3403]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6303)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3376, -0.5585], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2214, 0.3347], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2214, 0.3347]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6403)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3319, -0.5667], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2247, 0.3291], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2247, 0.3291]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6503)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3262, -0.5748], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2279, 0.3233], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2279, 0.3233]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6603)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3204, -0.5828], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2312, 0.3175], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2312, 0.3175]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6703)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3145, -0.5906], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2343, 0.3116], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2343, 0.3116]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6803)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3086, -0.5983], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2374, 0.3056], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2374, 0.3056]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6903)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.3025, -0.6058], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2404, 0.2995], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2404, 0.2995]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7004)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2964, -0.6132], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2434, 0.2934], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2434, 0.2934]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7104)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2902, -0.6204], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2463, 0.2872], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2463, 0.2872]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7204)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2840, -0.6274], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2491, 0.2809], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2491, 0.2809]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7304)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2777, -0.6343], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2519, 0.2746], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2519, 0.2746]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7404)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2713, -0.6410], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2546, 0.2681], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2546, 0.2681]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7504)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2648, -0.6476], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2573, 0.2617], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2573, 0.2617]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7604)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2583, -0.6539], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2598, 0.2551], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2598, 0.2551]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7704)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2517, -0.6600], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2624, 0.2485], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2624, 0.2485]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7804)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2451, -0.6660], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2648, 0.2418], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2648, 0.2418]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7904)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2384, -0.6717], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2672, 0.2351], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2672, 0.2351]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8004)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2316, -0.6773], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2695, 0.2284], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2695, 0.2284]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8104)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2248, -0.6826], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2718, 0.2215], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2718, 0.2215]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8204)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2179, -0.6877], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2740, 0.2146], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2740, 0.2146]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8304)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2110, -0.6926], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2761, 0.2077], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2761, 0.2077]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8404)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2040, -0.6972], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2781, 0.2007], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2781, 0.2007]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8504)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1970, -0.7017], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2801, 0.1937], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2801, 0.1937]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8604)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1900, -0.7059], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2820, 0.1867], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2820, 0.1867]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8704)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1829, -0.7098], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2838, 0.1796], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2838, 0.1796]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8804)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1758, -0.7135], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2856, 0.1724], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2856, 0.1724]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8905)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1686, -0.7170], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2873, 0.1652], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2873, 0.1652]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9005)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1614, -0.7202], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2889, 0.1580], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2889, 0.1580]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9105)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1542, -0.7232], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2904, 0.1508], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2904, 0.1508]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9205)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1469, -0.7259], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2919, 0.1435], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2919, 0.1435]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9305)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1396, -0.7283], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2933, 0.1363], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2933, 0.1363]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9405)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1323, -0.7305], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2946, 0.1289], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2946, 0.1289]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9505)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1250, -0.7324], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2958, 0.1216], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2958, 0.1216]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9605)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1176, -0.7341], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2970, 0.1143], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2970, 0.1143]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9705)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1103, -0.7355], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2981, 0.1069], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2981, 0.1069]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9805)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1029, -0.7366], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2992, 0.0995], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2992, 0.0995]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9905)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0955, -0.7374], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3001, 0.0922], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3001, 0.0922]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0005)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0881, -0.7380], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3010, 0.0848], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3010, 0.0848]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0105)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0807, -0.7383], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3018, 0.0774], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3018, 0.0774]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0205)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0734, -0.7384], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3025, 0.0700], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3025, 0.0700]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0305)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0660, -0.7382], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3032, 0.0626], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3032, 0.0626]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0405)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0586, -0.7377], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3038, 0.0552], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3038, 0.0552]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0505)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0512, -0.7370], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3043, 0.0479], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3043, 0.0479]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0605)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0438, -0.7359], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3047, 0.0405], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3047, 0.0405]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0705)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0365, -0.7346], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3051, 0.0332], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3051, 0.0332]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0805)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0291, -0.7333], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3054, 0.0258], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3054, 0.0258]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0906)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0218, -0.7316], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3056, 0.0185], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3056, 0.0185]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1006)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0145, -0.7296], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3058, 0.0112], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3058, 0.0112]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1106)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0072, -0.7273], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.3058, 0.0039], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.3058, 0.0039]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1206)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-6.3442e-05, -7.2463e-01], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3058, -0.0033], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3058, -0.0033]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1306)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0073, -0.7217], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3058, -0.0106], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3058, -0.0106]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1406)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0145, -0.7185], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3056, -0.0177], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3056, -0.0177]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1506)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0217, -0.7150], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3054, -0.0249], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3054, -0.0249]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1606)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0288, -0.7113], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3051, -0.0320], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3051, -0.0320]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1706)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0359, -0.7074], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3047, -0.0391], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3047, -0.0391]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1806)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0430, -0.7032], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3043, -0.0461], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3043, -0.0461]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1906)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0500, -0.6989], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3038, -0.0531], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3038, -0.0531]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.2006)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0569, -0.6943], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3032, -0.0601], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3032, -0.0601]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.2106)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0639, -0.6895], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3026, -0.0670], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3026, -0.0670]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.2206)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0707, -0.6845], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3019, -0.0738], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3019, -0.0738]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.2306)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0775, -0.6792], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3011, -0.0806], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3011, -0.0806]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.2406)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0843, -0.6737], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.3003, -0.0873], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.3003, -0.0873]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.2506)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0910, -0.6680], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2994, -0.0940], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2994, -0.0940]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.2606)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0977, -0.6621], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2984, -0.1007], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2984, -0.1007]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.2706)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1043, -0.6559], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2973, -0.1072], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2973, -0.1072]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.2806)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1108, -0.6493], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2962, -0.1137], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2962, -0.1137]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.2907)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1173, -0.6427], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2951, -0.1201], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2951, -0.1201]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.3007)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1236, -0.6360], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2938, -0.1265], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2938, -0.1265]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.3107)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1300, -0.6291], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2925, -0.1328], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2925, -0.1328]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.3207)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1362, -0.6219], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2912, -0.1390], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2912, -0.1390]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.3307)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1424, -0.6145], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2897, -0.1452], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2897, -0.1452]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.3407)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1485, -0.6069], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2883, -0.1512], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2883, -0.1512]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.3507)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1546, -0.5991], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2867, -0.1572], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2867, -0.1572]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.3607)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1605, -0.5912], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2851, -0.1632], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2851, -0.1632]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.3707)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1664, -0.5831], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2834, -0.1690], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2834, -0.1690]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.3807)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1722, -0.5749], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2817, -0.1747], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2817, -0.1747]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.3907)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1779, -0.5665], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2799, -0.1804], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2799, -0.1804]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.4007)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1835, -0.5580], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2781, -0.1860], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2781, -0.1860]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.4107)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1890, -0.5494], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2762, -0.1915], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2762, -0.1915]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.4207)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1945, -0.5406], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2743, -0.1969], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2743, -0.1969]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.4307)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1999, -0.5318], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2723, -0.2022], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2723, -0.2022]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.4407)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2051, -0.5228], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2702, -0.2074], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2702, -0.2074]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.4507)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2103, -0.5137], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2681, -0.2126], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2681, -0.2126]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.4607)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2154, -0.5045], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2659, -0.2176], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2659, -0.2176]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.4707)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2204, -0.4952], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2637, -0.2226], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2637, -0.2226]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.4808)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2253, -0.4858], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2615, -0.2274], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2615, -0.2274]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.4908)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2301, -0.4763], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2592, -0.2322], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2592, -0.2322]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.5008)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2349, -0.4666], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2568, -0.2369], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2568, -0.2369]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.5108)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2395, -0.4569], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2544, -0.2415], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2544, -0.2415]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.5208)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2440, -0.4472], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2520, -0.2459], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2520, -0.2459]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.5308)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2484, -0.4375], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2495, -0.2503], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2495, -0.2503]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.5408)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2528, -0.4277], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2470, -0.2546], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2470, -0.2546]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.5508)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2570, -0.4178], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2444, -0.2588], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2444, -0.2588]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.5608)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2611, -0.4079], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2418, -0.2628], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2418, -0.2628]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.5708)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2651, -0.3979], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2391, -0.2668], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2391, -0.2668]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.5808)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2691, -0.3878], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2365, -0.2707], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2365, -0.2707]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.5908)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2729, -0.3777], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2337, -0.2745], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2337, -0.2745]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.6008)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2766, -0.3675], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2310, -0.2782], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2310, -0.2782]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.6108)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2803, -0.3574], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2282, -0.2817], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2282, -0.2817]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.6208)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2838, -0.3472], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2253, -0.2852], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2253, -0.2852]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.6308)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2872, -0.3370], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2224, -0.2886], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2224, -0.2886]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.6408)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2905, -0.3269], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2195, -0.2919], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2195, -0.2919]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.6508)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2938, -0.3167], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2166, -0.2950], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2166, -0.2950]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.6608)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2969, -0.3066], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2136, -0.2981], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2136, -0.2981]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.6708)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2999, -0.2965], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2106, -0.3011], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2106, -0.3011]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.6809)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3028, -0.2864], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2076, -0.3039], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2076, -0.3039]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.6909)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3056, -0.2763], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2045, -0.3067], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2045, -0.3067]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.7009)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3084, -0.2663], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2015, -0.3093], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2015, -0.3093]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.7109)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3110, -0.2563], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1983, -0.3119], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1983, -0.3119]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.7209)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3135, -0.2464], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1952, -0.3144], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1952, -0.3144]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.7309)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3159, -0.2365], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1920, -0.3167], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1920, -0.3167]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.7409)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3182, -0.2267], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1889, -0.3190], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1889, -0.3190]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.7509)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3204, -0.2169], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1857, -0.3212], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1857, -0.3212]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.7609)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3226, -0.2072], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1824, -0.3233], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1824, -0.3233]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.7709)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3246, -0.1975], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1792, -0.3252], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1792, -0.3252]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.7809)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3265, -0.1879], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1759, -0.3271], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1759, -0.3271]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.7909)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3284, -0.1784], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1726, -0.3289], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1726, -0.3289]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.8009)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3301, -0.1689], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1693, -0.3306], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1693, -0.3306]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.8109)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3318, -0.1595], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1660, -0.3322], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1660, -0.3322]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.8209)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3333, -0.1501], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1627, -0.3337], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1627, -0.3337]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.8309)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3348, -0.1409], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1593, -0.3351], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1593, -0.3351]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.8409)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3361, -0.1317], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1560, -0.3364], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1560, -0.3364]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.8509)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3374, -0.1226], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1526, -0.3376], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1526, -0.3376]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.8609)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3386, -0.1136], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1492, -0.3388], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1492, -0.3388]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.8709)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3397, -0.1047], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1458, -0.3398], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1458, -0.3398]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.8810)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3407, -0.0959], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1424, -0.3408], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1424, -0.3408]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.8910)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3416, -0.0872], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1390, -0.3417], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1390, -0.3417]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.9010)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3424, -0.0786], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1355, -0.3424], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1355, -0.3424]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.9110)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3432, -0.0701], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1321, -0.3431], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1321, -0.3431]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.9210)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3438, -0.0617], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1287, -0.3438], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1287, -0.3438]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.9310)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3444, -0.0535], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1252, -0.3443], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1252, -0.3443]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.9410)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3449, -0.0453], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1218, -0.3447], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1218, -0.3447]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.9510)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3453, -0.0373], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1183, -0.3451], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1183, -0.3451]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.9610)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3457, -0.0294], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1149, -0.3454], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1149, -0.3454]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.9710)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3459, -0.0216], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1114, -0.3456], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1114, -0.3456]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.9810)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3461, -0.0139], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1079, -0.3458], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1079, -0.3458]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.9910)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3462, -0.0063], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1045, -0.3458], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1045, -0.3458]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.0010)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3462,  0.0011], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1010, -0.3458], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1010, -0.3458]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.0110)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3462,  0.0085], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0975, -0.3457], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0975, -0.3457]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.0210)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3461,  0.0157], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0941, -0.3456], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0941, -0.3456]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.0310)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3459,  0.0228], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0906, -0.3454], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0906, -0.3454]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.0410)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3456,  0.0298], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0872, -0.3451], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0872, -0.3451]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.0510)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3453,  0.0367], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0837, -0.3447], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0837, -0.3447]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.0610)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3449,  0.0434], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0803, -0.3443], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0803, -0.3443]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.0710)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3444,  0.0500], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0768, -0.3438], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0768, -0.3438]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.0811)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3439,  0.0565], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0734, -0.3432], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0734, -0.3432]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.0911)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3433,  0.0629], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0699, -0.3426], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0699, -0.3426]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.1011)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3427,  0.0691], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0665, -0.3419], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0665, -0.3419]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.1111)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3419,  0.0751], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0631, -0.3411], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0631, -0.3411]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.1211)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3412,  0.0811], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0597, -0.3403], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0597, -0.3403]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.1311)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3403,  0.0869], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0563, -0.3394], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0563, -0.3394]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.1411)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3394,  0.0928], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0529, -0.3385], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0529, -0.3385]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.1511)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3385,  0.0984], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0495, -0.3375], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0495, -0.3375]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.1611)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3375,  0.1038], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0461, -0.3365], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0461, -0.3365]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.1711)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3364,  0.1090], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0427, -0.3354], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0427, -0.3354]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.1811)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3353,  0.1142], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0394, -0.3342], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0394, -0.3342]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.1911)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3341,  0.1191], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0360, -0.3331], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0360, -0.3331]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.2011)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3329,  0.1240], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0327, -0.3318], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0327, -0.3318]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.2111)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3317,  0.1287], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0294, -0.3305], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0294, -0.3305]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.2211)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3303,  0.1334], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0261, -0.3292], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0261, -0.3292]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.2311)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3290,  0.1380], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0228, -0.3278], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0228, -0.3278]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.2411)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3276,  0.1425], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0195, -0.3264], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0195, -0.3264]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.2511)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3261,  0.1469], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0163, -0.3249], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0163, -0.3249]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.2611)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3247,  0.1512], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0130, -0.3234], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0130, -0.3234]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.2712)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3231,  0.1552], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0098, -0.3219], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0098, -0.3219]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.2812)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3215,  0.1591], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0066, -0.3203], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0066, -0.3203]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.2912)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3199,  0.1629], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0034, -0.3186], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0034, -0.3186]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.3012)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3183,  0.1665], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.0002, -0.3170], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.0002, -0.3170]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.3112)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3166,  0.1701], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9970, -0.3153], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9970, -0.3153]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.3212)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3149,  0.1735], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9939, -0.3135], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9939, -0.3135]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.3312)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3131,  0.1768], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9907, -0.3118], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9907, -0.3118]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.3412)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3114,  0.1800], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9876, -0.3100], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9876, -0.3100]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.3512)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3095,  0.1832], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9845, -0.3081], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9845, -0.3081]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.3612)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3077,  0.1863], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9814, -0.3063], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9814, -0.3063]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.3712)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3058,  0.1893], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9784, -0.3044], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9784, -0.3044]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.3812)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3039,  0.1922], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9753, -0.3024], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9753, -0.3024]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.3912)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3020,  0.1949], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9723, -0.3005], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9723, -0.3005]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.4012)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.3000,  0.1975], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9693, -0.2985], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9693, -0.2985]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.4112)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2980,  0.2001], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9663, -0.2965], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9663, -0.2965]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.4212)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2960,  0.2025], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9634, -0.2945], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9634, -0.2945]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.4312)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2940,  0.2048], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9604, -0.2924], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9604, -0.2924]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.4412)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2919,  0.2070], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9575, -0.2904], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9575, -0.2904]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.4512)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2898,  0.2091], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9546, -0.2883], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9546, -0.2883]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.4612)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2877,  0.2111], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9517, -0.2862], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9517, -0.2862]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.4713)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2856,  0.2130], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9489, -0.2840], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9489, -0.2840]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.4813)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2835,  0.2149], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9460, -0.2819], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9460, -0.2819]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.4913)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2813,  0.2166], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9432, -0.2797], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9432, -0.2797]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.5013)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2791,  0.2183], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9404, -0.2775], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9404, -0.2775]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.5113)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2769,  0.2200], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9377, -0.2753], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9377, -0.2753]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.5213)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2747,  0.2216], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9349, -0.2731], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9349, -0.2731]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.5313)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2725,  0.2230], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9322, -0.2709], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9322, -0.2709]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.5413)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2703,  0.2243], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9295, -0.2686], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9295, -0.2686]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.5513)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2680,  0.2254], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9268, -0.2664], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9268, -0.2664]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.5613)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2658,  0.2265], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9241, -0.2641], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9241, -0.2641]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.5713)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2635,  0.2275], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9215, -0.2618], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9215, -0.2618]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.5813)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2612,  0.2285], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9189, -0.2596], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9189, -0.2596]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.5913)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2589,  0.2293], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9163, -0.2573], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9163, -0.2573]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.6013)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2566,  0.2302], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9137, -0.2550], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9137, -0.2550]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.6113)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2543,  0.2310], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9112, -0.2526], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9112, -0.2526]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.6213)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2520,  0.2317], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9087, -0.2503], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9087, -0.2503]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.6313)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2497,  0.2323], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9062, -0.2480], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9062, -0.2480]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.6413)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2473,  0.2328], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9037, -0.2457], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9037, -0.2457]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.6513)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2450,  0.2333], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.9012, -0.2433], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.9012, -0.2433]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.6613)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2427,  0.2337], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8988, -0.2410], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8988, -0.2410]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.6714)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2403,  0.2340], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8964, -0.2387], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8964, -0.2387]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.6814)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2380,  0.2344], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8940, -0.2363], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8940, -0.2363]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.6914)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2356,  0.2346], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8917, -0.2340], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8917, -0.2340]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.7014)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2333,  0.2348], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8893, -0.2316], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8893, -0.2316]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.7114)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2309,  0.2349], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8870, -0.2293], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8870, -0.2293]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.7214)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2286,  0.2349], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8847, -0.2269], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8847, -0.2269]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.7314)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2262,  0.2348], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8825, -0.2246], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8825, -0.2246]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.7414)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2239,  0.2347], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8802, -0.2222], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8802, -0.2222]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.7514)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2215,  0.2346], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8780, -0.2199], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8780, -0.2199]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.7614)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2192,  0.2344], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8758, -0.2175], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8758, -0.2175]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.7714)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2168,  0.2341], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8737, -0.2152], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8737, -0.2152]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.7814)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2145,  0.2338], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8715, -0.2128], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8715, -0.2128]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.7914)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2121,  0.2336], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8694, -0.2105], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8694, -0.2105]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.8014)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2098,  0.2332], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8673, -0.2082], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8673, -0.2082]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.8114)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2075,  0.2329], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8652, -0.2058], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8652, -0.2058]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.8214)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2051,  0.2324], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8632, -0.2035], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8632, -0.2035]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.8314)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2028,  0.2319], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8611, -0.2012], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8611, -0.2012]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.8414)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.2005,  0.2314], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8591, -0.1989], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8591, -0.1989]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.8514)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1982,  0.2308], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8571, -0.1966], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8571, -0.1966]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.8615)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1959,  0.2301], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8552, -0.1943], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8552, -0.1943]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.8715)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1936,  0.2294], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8532, -0.1920], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8532, -0.1920]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.8815)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1913,  0.2287], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8513, -0.1897], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8513, -0.1897]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.8915)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1890,  0.2279], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8494, -0.1874], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8494, -0.1874]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.9015)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1867,  0.2270], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8476, -0.1851], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8476, -0.1851]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.9115)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1844,  0.2262], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8457, -0.1829], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8457, -0.1829]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.9215)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1822,  0.2254], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8439, -0.1806], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8439, -0.1806]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.9315)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1799,  0.2247], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8421, -0.1784], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8421, -0.1784]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.9415)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1777,  0.2240], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8403, -0.1761], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8403, -0.1761]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.9515)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1754,  0.2230], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8386, -0.1739], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8386, -0.1739]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.9615)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1732,  0.2220], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8368, -0.1717], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8368, -0.1717]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.9715)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1710,  0.2209], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8351, -0.1695], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8351, -0.1695]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.9815)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1688,  0.2198], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8334, -0.1673], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8334, -0.1673]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(2.9915)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1666,  0.2187], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8318, -0.1651], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8318, -0.1651]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.0015)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1644,  0.2176], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8301, -0.1629], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8301, -0.1629]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.0115)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1622,  0.2165], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8285, -0.1607], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8285, -0.1607]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.0215)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1601,  0.2156], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8269, -0.1586], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8269, -0.1586]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.0315)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1579,  0.2146], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8253, -0.1564], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8253, -0.1564]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.0415)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1558,  0.2137], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8238, -0.1543], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8238, -0.1543]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.0515)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1536,  0.2127], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8222, -0.1522], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8222, -0.1522]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.0616)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1515,  0.2116], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8207, -0.1501], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8207, -0.1501]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.0716)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1494,  0.2103], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8192, -0.1479], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8192, -0.1479]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.0816)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1473,  0.2087], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8177, -0.1459], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8177, -0.1459]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.0916)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1452,  0.2073], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8163, -0.1438], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8163, -0.1438]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.1016)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1431,  0.2064], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8149, -0.1417], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8149, -0.1417]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.1116)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1411,  0.2055], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8134, -0.1397], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8134, -0.1397]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.1216)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1390,  0.2042], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8121, -0.1376], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8121, -0.1376]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.1316)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1370,  0.2030], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8107, -0.1356], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8107, -0.1356]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.1416)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1350,  0.2018], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8093, -0.1336], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8093, -0.1336]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.1516)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1330,  0.2006], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8080, -0.1316], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8080, -0.1316]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.1616)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1310,  0.1993], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8067, -0.1296], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8067, -0.1296]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.1716)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1290,  0.1980], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8054, -0.1276], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8054, -0.1276]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.1816)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1270,  0.1966], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8041, -0.1256], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8041, -0.1256]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.1916)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1250,  0.1953], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8029, -0.1237], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8029, -0.1237]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.2016)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1231,  0.1940], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8017, -0.1217], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8017, -0.1217]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.2116)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1211,  0.1927], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.8004, -0.1198], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.8004, -0.1198]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.2216)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1192,  0.1915], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7992, -0.1179], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7992, -0.1179]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.2316)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1173,  0.1903], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7981, -0.1160], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7981, -0.1160]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.2416)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1154,  0.1891], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7969, -0.1141], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7969, -0.1141]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.2516)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1135,  0.1878], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7958, -0.1122], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7958, -0.1122]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.2617)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1116,  0.1865], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7947, -0.1103], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7947, -0.1103]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.2717)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1098,  0.1852], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7936, -0.1085], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7936, -0.1085]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.2817)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1079,  0.1839], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7925, -0.1066], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7925, -0.1066]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.2917)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1061,  0.1826], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7914, -0.1048], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7914, -0.1048]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.3017)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1043,  0.1813], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7904, -0.1030], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7904, -0.1030]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.3117)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1025,  0.1800], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7894, -0.1012], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7894, -0.1012]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.3217)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1007,  0.1787], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7883, -0.0994], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7883, -0.0994]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.3317)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0989,  0.1774], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7874, -0.0976], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7874, -0.0976]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.3417)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0971,  0.1761], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7864, -0.0959], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7864, -0.0959]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.3517)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0954,  0.1749], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7854, -0.0941], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7854, -0.0941]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.3617)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0936,  0.1737], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7845, -0.0924], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7845, -0.0924]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.3717)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0919,  0.1725], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7836, -0.0907], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7836, -0.0907]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.3817)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0902,  0.1713], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7827, -0.0890], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7827, -0.0890]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.3917)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0885,  0.1701], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7818, -0.0873], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7818, -0.0873]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.4017)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0868,  0.1689], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7809, -0.0856], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7809, -0.0856]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.4117)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0851,  0.1677], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7801, -0.0839], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7801, -0.0839]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.4217)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0834,  0.1665], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7792, -0.0822], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7792, -0.0822]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.4317)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0818,  0.1653], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7784, -0.0806], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7784, -0.0806]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.4417)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0801,  0.1640], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7776, -0.0789], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7776, -0.0789]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.4517)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0785,  0.1625], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7768, -0.0773], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7768, -0.0773]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.4618)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0768,  0.1618], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7761, -0.0757], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7761, -0.0757]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.4718)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0752,  0.1609], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7753, -0.0741], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7753, -0.0741]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.4818)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0736,  0.1597], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7746, -0.0725], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7746, -0.0725]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.4918)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0720,  0.1586], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7739, -0.0709], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7739, -0.0709]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.5018)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0705,  0.1574], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7731, -0.0693], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7731, -0.0693]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.5118)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0689,  0.1562], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7725, -0.0678], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7725, -0.0678]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.5218)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0673,  0.1551], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7718, -0.0662], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7718, -0.0662]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.5318)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0658,  0.1540], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7711, -0.0647], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7711, -0.0647]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.5418)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0642,  0.1530], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7705, -0.0631], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7705, -0.0631]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.5518)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0627,  0.1520], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7699, -0.0616], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7699, -0.0616]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.5618)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0612,  0.1510], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7692, -0.0601], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7692, -0.0601]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.5718)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0597,  0.1500], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7686, -0.0586], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7686, -0.0586]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.5818)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0582,  0.1490], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7681, -0.0571], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7681, -0.0571]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.5918)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0567,  0.1480], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7675, -0.0556], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7675, -0.0556]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.6018)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0552,  0.1470], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7669, -0.0542], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7669, -0.0542]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.6118)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0537,  0.1461], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7664, -0.0527], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7664, -0.0527]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.6218)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0523,  0.1451], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7659, -0.0512], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7659, -0.0512]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.6318)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0508,  0.1438], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7654, -0.0498], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7654, -0.0498]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.6418)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0494,  0.1431], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7649, -0.0484], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7649, -0.0484]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.6519)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0480,  0.1424], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7644, -0.0469], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7644, -0.0469]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.6619)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0466,  0.1415], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7639, -0.0455], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7639, -0.0455]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.6719)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0451,  0.1406], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7635, -0.0441], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7635, -0.0441]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.6819)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0437,  0.1398], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7630, -0.0427], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7630, -0.0427]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.6919)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0423,  0.1390], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7626, -0.0413], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7626, -0.0413]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.7019)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0410,  0.1384], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7622, -0.0399], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7622, -0.0399]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.7119)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0396,  0.1377], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7618, -0.0386], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7618, -0.0386]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.7219)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0382,  0.1370], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7614, -0.0372], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7614, -0.0372]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.7319)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0368,  0.1363], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7611, -0.0358], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7611, -0.0358]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.7419)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0355,  0.1356], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7607, -0.0345], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7607, -0.0345]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.7519)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0341,  0.1348], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7604, -0.0331], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7604, -0.0331]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.7619)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0328,  0.1343], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7600, -0.0318], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7600, -0.0318]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.7719)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0314,  0.1338], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7597, -0.0304], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7597, -0.0304]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.7819)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0301,  0.1333], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7594, -0.0291], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7594, -0.0291]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.7919)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0288,  0.1327], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7591, -0.0278], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7591, -0.0278]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.8019)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0274,  0.1322], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7589, -0.0265], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7589, -0.0265]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.8119)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0261,  0.1318], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7586, -0.0251], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7586, -0.0251]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.8219)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0248,  0.1313], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7584, -0.0238], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7584, -0.0238]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.8319)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0235,  0.1308], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7581, -0.0225], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7581, -0.0225]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.8419)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0222,  0.1303], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7579, -0.0212], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7579, -0.0212]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.8520)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0209,  0.1299], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7577, -0.0199], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7577, -0.0199]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.8620)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0196,  0.1295], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7575, -0.0186], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7575, -0.0186]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.8720)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0183,  0.1290], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7573, -0.0173], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7573, -0.0173]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.8820)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0170,  0.1285], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7571, -0.0160], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7571, -0.0160]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.8920)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0157,  0.1281], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7570, -0.0148], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7570, -0.0148]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.9020)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0144,  0.1277], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7568, -0.0135], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7568, -0.0135]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.9120)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0132,  0.1272], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7567, -0.0122], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7567, -0.0122]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.9220)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0119,  0.1270], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7566, -0.0109], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7566, -0.0109]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.9320)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0106,  0.1267], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7565, -0.0097], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7565, -0.0097]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.9420)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0093,  0.1264], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7564, -0.0084], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7564, -0.0084]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.9520)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0081,  0.1261], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7563, -0.0071], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7563, -0.0071]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.9620)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0068,  0.1258], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7562, -0.0059], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7562, -0.0059]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.9720)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0056,  0.1256], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7562, -0.0046], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7562, -0.0046]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.9820)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0043,  0.1254], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7561, -0.0034], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7561, -0.0034]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(3.9920)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0031,  0.1252], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7561, -0.0021], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7561, -0.0021]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.0020)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0018,  0.1251], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 0.7561, -0.0009], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 0.7561, -0.0009]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.0120)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0006,  0.1250], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([7.5609e-01, 3.7742e-04], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[7.5609e-01, 3.7742e-04]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.0220)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0007, 0.1250], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7561, 0.0016], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7561, 0.0016]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.0320)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0019, 0.1248], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7561, 0.0029], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7561, 0.0029]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.0420)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0032, 0.1247], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7561, 0.0041], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7561, 0.0041]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.0521)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0044, 0.1246], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7562, 0.0054], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7562, 0.0054]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.0621)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0057, 0.1246], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7562, 0.0066], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7562, 0.0066]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.0721)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0069, 0.1246], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7563, 0.0079], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7563, 0.0079]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.0821)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0082, 0.1245], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7564, 0.0091], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7564, 0.0091]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.0921)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0094, 0.1245], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7565, 0.0104], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7565, 0.0104]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.1021)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0107, 0.1245], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7566, 0.0116], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7566, 0.0116]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.1121)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0119, 0.1246], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7567, 0.0128], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7567, 0.0128]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.1221)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0132, 0.1247], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7569, 0.0141], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7569, 0.0141]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.1321)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0144, 0.1248], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7570, 0.0153], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7570, 0.0153]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.1421)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0157, 0.1248], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7572, 0.0166], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7572, 0.0166]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.1521)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0169, 0.1249], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7573, 0.0178], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7573, 0.0178]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.1621)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0182, 0.1250], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7575, 0.0191], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7575, 0.0191]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.1721)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0194, 0.1251], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7577, 0.0203], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7577, 0.0203]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.1821)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0207, 0.1254], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7579, 0.0216], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7579, 0.0216]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.1921)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0219, 0.1256], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7581, 0.0229], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7581, 0.0229]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.2021)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0232, 0.1258], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7584, 0.0241], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7584, 0.0241]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.2121)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0244, 0.1262], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7586, 0.0254], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7586, 0.0254]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.2221)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0257, 0.1265], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7589, 0.0266], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7589, 0.0266]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.2321)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0270, 0.1267], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7591, 0.0279], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7591, 0.0279]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.2422)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0282, 0.1270], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7594, 0.0292], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7594, 0.0292]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.2522)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0295, 0.1274], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7597, 0.0305], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7597, 0.0305]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.2622)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0308, 0.1277], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7600, 0.0317], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7600, 0.0317]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.2722)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0321, 0.1281], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7603, 0.0330], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7603, 0.0330]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.2822)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0334, 0.1284], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7607, 0.0343], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7607, 0.0343]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.2922)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0346, 0.1286], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7610, 0.0356], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7610, 0.0356]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.3022)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0359, 0.1290], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7614, 0.0369], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7614, 0.0369]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.3122)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0372, 0.1294], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7617, 0.0382], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7617, 0.0382]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.3222)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0385, 0.1298], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7621, 0.0395], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7621, 0.0395]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.3322)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0398, 0.1302], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7625, 0.0408], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7625, 0.0408]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.3422)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0411, 0.1307], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7629, 0.0421], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7629, 0.0421]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.3522)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0424, 0.1312], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7634, 0.0434], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7634, 0.0434]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.3622)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0438, 0.1317], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7638, 0.0447], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7638, 0.0447]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.3722)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0451, 0.1322], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7643, 0.0460], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7643, 0.0460]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.3822)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0464, 0.1328], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7647, 0.0474], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7647, 0.0474]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.3922)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0477, 0.1333], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7652, 0.0487], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7652, 0.0487]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.4022)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0491, 0.1338], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7657, 0.0500], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7657, 0.0500]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.4122)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0504, 0.1344], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7662, 0.0514], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7662, 0.0514]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.4222)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0518, 0.1351], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7667, 0.0527], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7667, 0.0527]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.4322)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0531, 0.1357], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7672, 0.0541], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7672, 0.0541]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.4423)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0545, 0.1362], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7678, 0.0555], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7678, 0.0555]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.4523)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0558, 0.1366], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7683, 0.0568], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7683, 0.0568]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.4623)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0572, 0.1372], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7689, 0.0582], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7689, 0.0582]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.4723)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0586, 0.1378], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7695, 0.0596], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7695, 0.0596]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.4823)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0600, 0.1384], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7701, 0.0610], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7701, 0.0610]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.4923)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0614, 0.1391], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7707, 0.0623], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7707, 0.0623]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.5023)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0627, 0.1397], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7713, 0.0637], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7713, 0.0637]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.5123)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0641, 0.1404], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7720, 0.0652], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7720, 0.0652]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.5223)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0656, 0.1410], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7726, 0.0666], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7726, 0.0666]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.5323)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0670, 0.1416], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7733, 0.0680], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7733, 0.0680]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.5423)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0684, 0.1422], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7740, 0.0694], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7740, 0.0694]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.5523)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0698, 0.1429], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7747, 0.0708], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7747, 0.0708]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.5623)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0712, 0.1436], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7754, 0.0723], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7754, 0.0723]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.5723)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0727, 0.1443], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7761, 0.0737], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7761, 0.0737]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.5823)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0741, 0.1449], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7769, 0.0752], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7769, 0.0752]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.5923)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0756, 0.1454], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7776, 0.0766], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7776, 0.0766]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.6023)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0770, 0.1460], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7784, 0.0781], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7784, 0.0781]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.6123)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0785, 0.1467], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7792, 0.0795], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7792, 0.0795]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.6223)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0800, 0.1474], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7800, 0.0810], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7800, 0.0810]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.6323)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0815, 0.1482], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7808, 0.0825], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7808, 0.0825]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.6424)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0829, 0.1489], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7816, 0.0840], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7816, 0.0840]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.6524)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0844, 0.1496], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7825, 0.0855], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7825, 0.0855]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.6624)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0859, 0.1503], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7833, 0.0870], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7833, 0.0870]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.6724)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0874, 0.1510], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7842, 0.0885], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7842, 0.0885]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.6824)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0890, 0.1517], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7851, 0.0900], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7851, 0.0900]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.6924)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0905, 0.1523], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7860, 0.0915], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7860, 0.0915]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.7024)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0920, 0.1530], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7869, 0.0931], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7869, 0.0931]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.7124)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0935, 0.1536], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7879, 0.0946], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7879, 0.0946]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.7224)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0951, 0.1542], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7888, 0.0962], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7888, 0.0962]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.7324)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0966, 0.1548], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7898, 0.0977], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7898, 0.0977]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.7424)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0982, 0.1556], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7908, 0.0993], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7908, 0.0993]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.7524)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.0997, 0.1564], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7918, 0.1008], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7918, 0.1008]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.7624)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1013, 0.1570], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7928, 0.1024], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7928, 0.1024]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.7724)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1029, 0.1576], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7938, 0.1040], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7938, 0.1040]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.7824)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1045, 0.1583], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7949, 0.1056], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7949, 0.1056]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.7924)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1061, 0.1589], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7959, 0.1071], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7959, 0.1071]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.8024)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1077, 0.1596], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7970, 0.1087], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7970, 0.1087]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.8124)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1093, 0.1602], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7981, 0.1103], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7981, 0.1103]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.8224)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1109, 0.1607], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.7992, 0.1120], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.7992, 0.1120]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.8324)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1125, 0.1613], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8003, 0.1136], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8003, 0.1136]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.8425)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1141, 0.1619], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8015, 0.1152], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8015, 0.1152]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.8525)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1157, 0.1624], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8026, 0.1168], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8026, 0.1168]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.8625)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1173, 0.1630], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8038, 0.1184], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8038, 0.1184]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.8725)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1190, 0.1636], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8050, 0.1201], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8050, 0.1201]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.8825)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1206, 0.1641], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8062, 0.1217], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8062, 0.1217]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.8925)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1223, 0.1646], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8074, 0.1234], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8074, 0.1234]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.9025)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1239, 0.1651], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8087, 0.1250], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8087, 0.1250]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.9125)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1256, 0.1656], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8099, 0.1267], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8099, 0.1267]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.9225)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1272, 0.1661], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8112, 0.1283], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8112, 0.1283]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.9325)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1289, 0.1667], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8125, 0.1300], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8125, 0.1300]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.9425)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1306, 0.1672], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8138, 0.1317], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8138, 0.1317]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.9525)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1322, 0.1678], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8151, 0.1334], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8151, 0.1334]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.9625)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1339, 0.1684], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8164, 0.1350], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8164, 0.1350]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.9725)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1356, 0.1687], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8178, 0.1367], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8178, 0.1367]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.9825)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1373, 0.1690], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8192, 0.1384], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8192, 0.1384]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(4.9925)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1390, 0.1693], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8206, 0.1401], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8206, 0.1401]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.0025)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1407, 0.1697], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8220, 0.1418], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8220, 0.1418]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.0125)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1424, 0.1701], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8234, 0.1435], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8234, 0.1435]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.0225)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1441, 0.1704], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8248, 0.1452], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8248, 0.1452]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.0326)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1458, 0.1707], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8263, 0.1469], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8263, 0.1469]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.0426)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1475, 0.1710], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8278, 0.1486], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8278, 0.1486]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.0526)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1492, 0.1714], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8293, 0.1504], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8293, 0.1504]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.0626)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1509, 0.1716], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8308, 0.1521], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8308, 0.1521]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.0726)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1527, 0.1719], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8323, 0.1538], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8323, 0.1538]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.0826)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1544, 0.1721], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8338, 0.1555], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8338, 0.1555]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.0926)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1561, 0.1723], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8354, 0.1572], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8354, 0.1572]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.1026)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1578, 0.1725], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8370, 0.1590], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8370, 0.1590]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.1126)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1596, 0.1725], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8386, 0.1607], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8386, 0.1607]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.1226)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1613, 0.1723], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8402, 0.1624], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8402, 0.1624]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.1326)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1630, 0.1722], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8418, 0.1641], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8418, 0.1641]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.1426)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1647, 0.1725], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8435, 0.1659], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8435, 0.1659]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.1526)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1665, 0.1727], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8451, 0.1676], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8451, 0.1676]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.1626)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1682, 0.1729], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8468, 0.1693], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8468, 0.1693]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.1726)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1699, 0.1728], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8485, 0.1711], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8485, 0.1711]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.1826)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1717, 0.1726], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8502, 0.1728], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8502, 0.1728]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.1926)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1734, 0.1724], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8520, 0.1745], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8520, 0.1745]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.2026)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1751, 0.1722], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8537, 0.1762], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8537, 0.1762]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.2126)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1768, 0.1721], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8555, 0.1779], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8555, 0.1779]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.2226)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1786, 0.1719], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8573, 0.1797], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8573, 0.1797]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.2327)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1803, 0.1718], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8591, 0.1814], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8591, 0.1814]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.2427)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1820, 0.1716], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8609, 0.1831], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8609, 0.1831]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.2527)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1837, 0.1714], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8627, 0.1848], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8627, 0.1848]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.2627)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1854, 0.1708], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8646, 0.1865], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8646, 0.1865]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.2727)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1871, 0.1702], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8665, 0.1882], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8665, 0.1882]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.2827)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1888, 0.1698], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8684, 0.1899], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8684, 0.1899]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.2927)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1905, 0.1694], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8703, 0.1916], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8703, 0.1916]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.3027)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1922, 0.1690], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8722, 0.1933], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8722, 0.1933]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.3127)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1939, 0.1685], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8741, 0.1950], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8741, 0.1950]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.3227)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1956, 0.1677], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8761, 0.1967], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8761, 0.1967]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.3327)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1973, 0.1668], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8781, 0.1984], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8781, 0.1984]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.3427)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.1989, 0.1661], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8801, 0.2000], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8801, 0.2000]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.3527)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2006, 0.1654], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8821, 0.2017], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8821, 0.2017]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.3627)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2023, 0.1647], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8841, 0.2033], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8841, 0.2033]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.3727)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2039, 0.1640], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8861, 0.2050], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8861, 0.2050]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.3827)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2055, 0.1633], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8882, 0.2066], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8882, 0.2066]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.3927)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2072, 0.1625], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8903, 0.2082], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8903, 0.2082]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.4027)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2088, 0.1615], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8923, 0.2098], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8923, 0.2098]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.4127)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2104, 0.1604], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8944, 0.2114], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8944, 0.2114]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.4227)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2120, 0.1590], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8966, 0.2130], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8966, 0.2130]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.4328)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2136, 0.1577], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.8987, 0.2146], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.8987, 0.2146]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.4428)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2152, 0.1564], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9009, 0.2162], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9009, 0.2162]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.4528)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2167, 0.1551], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9030, 0.2177], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9030, 0.2177]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.4628)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2183, 0.1538], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9052, 0.2193], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9052, 0.2193]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.4728)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2198, 0.1525], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9074, 0.2208], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9074, 0.2208]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.4828)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2213, 0.1511], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9096, 0.2223], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9096, 0.2223]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.4928)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2228, 0.1497], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9119, 0.2238], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9119, 0.2238]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.5028)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2243, 0.1483], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9141, 0.2253], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9141, 0.2253]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.5128)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2258, 0.1468], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9164, 0.2267], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9164, 0.2267]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.5228)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2272, 0.1452], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9186, 0.2282], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9186, 0.2282]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.5328)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2287, 0.1435], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9209, 0.2296], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9209, 0.2296]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.5428)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2301, 0.1418], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9232, 0.2311], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9232, 0.2311]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.5528)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2315, 0.1401], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9255, 0.2325], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9255, 0.2325]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.5628)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2329, 0.1382], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9279, 0.2338], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9279, 0.2338]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.5728)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2343, 0.1363], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9302, 0.2352], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9302, 0.2352]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.5828)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2356, 0.1344], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9326, 0.2365], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9326, 0.2365]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.5928)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2370, 0.1323], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9349, 0.2379], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9349, 0.2379]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.6028)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2383, 0.1302], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9373, 0.2392], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9373, 0.2392]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.6128)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2396, 0.1281], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9397, 0.2405], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9397, 0.2405]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.6229)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2409, 0.1259], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9421, 0.2417], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9421, 0.2417]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.6329)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2421, 0.1236], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9446, 0.2430], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9446, 0.2430]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.6429)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2433, 0.1213], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9470, 0.2442], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9470, 0.2442]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.6529)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2445, 0.1189], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9494, 0.2454], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9494, 0.2454]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.6629)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2457, 0.1164], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9519, 0.2465], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9519, 0.2465]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.6729)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2469, 0.1139], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9544, 0.2477], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9544, 0.2477]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.6829)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2480, 0.1113], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9568, 0.2488], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9568, 0.2488]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.6929)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2491, 0.1087], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9593, 0.2499], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9593, 0.2499]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.7029)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2502, 0.1060], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9618, 0.2509], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9618, 0.2509]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.7129)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2512, 0.1032], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9644, 0.2520], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9644, 0.2520]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.7229)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2522, 0.1004], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9669, 0.2530], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9669, 0.2530]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.7329)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2532, 0.0976], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9694, 0.2539], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9694, 0.2539]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.7429)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2542, 0.0946], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9720, 0.2549], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9720, 0.2549]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.7529)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2551, 0.0916], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9745, 0.2558], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9745, 0.2558]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.7629)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2560, 0.0886], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9771, 0.2567], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9771, 0.2567]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.7729)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2569, 0.0855], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9796, 0.2575], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9796, 0.2575]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.7829)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2577, 0.0823], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9822, 0.2584], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9822, 0.2584]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.7929)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2585, 0.0791], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9848, 0.2592], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9848, 0.2592]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.8029)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2593, 0.0758], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9874, 0.2599], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9874, 0.2599]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.8129)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2601, 0.0724], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9900, 0.2606], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9900, 0.2606]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.8230)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2608, 0.0690], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9926, 0.2613], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9926, 0.2613]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.8330)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2614, 0.0655], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9952, 0.2620], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9952, 0.2620]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.8430)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2621, 0.0620], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([0.9978, 0.2626], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[0.9978, 0.2626]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.8530)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2627, 0.0584], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0005, 0.2632], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0005, 0.2632]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.8630)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2632, 0.0547], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0031, 0.2637], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0031, 0.2637]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.8730)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2638, 0.0510], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0057, 0.2642], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0057, 0.2642]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.8830)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2643, 0.0472], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0084, 0.2647], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0084, 0.2647]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.8930)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2647, 0.0433], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0110, 0.2651], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0110, 0.2651]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.9030)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2651, 0.0394], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0137, 0.2655], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0137, 0.2655]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.9130)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2655, 0.0354], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0164, 0.2659], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0164, 0.2659]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.9230)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2658, 0.0314], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0190, 0.2662], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0190, 0.2662]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.9330)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2661, 0.0273], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0217, 0.2665], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0217, 0.2665]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.9430)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2664, 0.0231], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0243, 0.2667], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0243, 0.2667]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.9530)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2666, 0.0189], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0270, 0.2669], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0270, 0.2669]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.9630)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2668, 0.0146], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0297, 0.2671], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0297, 0.2671]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.9730)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2669, 0.0103], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0323, 0.2672], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0323, 0.2672]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.9830)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2670, 0.0059], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0350, 0.2672], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0350, 0.2672]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(5.9930)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([0.2670, 0.0014], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0377, 0.2672], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0377, 0.2672]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.0030)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2670, -0.0031], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0404, 0.2672], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0404, 0.2672]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.0130)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2670, -0.0076], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0430, 0.2671], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0430, 0.2671]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.0231)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2669, -0.0122], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0457, 0.2670], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0457, 0.2670]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.0331)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2667, -0.0169], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0484, 0.2668], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0484, 0.2668]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.0431)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2665, -0.0216], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0510, 0.2666], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0510, 0.2666]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.0531)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2663, -0.0263], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0537, 0.2664], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0537, 0.2664]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.0631)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2660, -0.0311], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0564, 0.2660], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0564, 0.2660]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.0731)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2657, -0.0360], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0590, 0.2657], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0590, 0.2657]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.0831)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2653, -0.0409], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0617, 0.2653], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0617, 0.2653]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.0931)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2648, -0.0458], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0643, 0.2648], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0643, 0.2648]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.1031)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2644, -0.0507], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0670, 0.2643], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0670, 0.2643]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.1131)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2638, -0.0557], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0696, 0.2637], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0696, 0.2637]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.1231)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2632, -0.0608], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0722, 0.2631], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0722, 0.2631]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.1331)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2626, -0.0658], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0749, 0.2625], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0749, 0.2625]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.1431)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2619, -0.0709], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0775, 0.2618], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0775, 0.2618]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.1531)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2612, -0.0761], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0801, 0.2610], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0801, 0.2610]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.1631)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2604, -0.0812], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0827, 0.2602], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0827, 0.2602]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.1731)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2596, -0.0864], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0853, 0.2593], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0853, 0.2593]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.1831)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2587, -0.0917], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0879, 0.2584], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0879, 0.2584]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.1931)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2577, -0.0970], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0905, 0.2574], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0905, 0.2574]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.2031)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2568, -0.1023], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0930, 0.2564], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0930, 0.2564]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.2132)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2557, -0.1076], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0956, 0.2553], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0956, 0.2553]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.2232)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2546, -0.1130], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.0981, 0.2542], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.0981, 0.2542]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.2332)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2535, -0.1185], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1007, 0.2530], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1007, 0.2530]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.2432)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2523, -0.1240], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1032, 0.2518], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1032, 0.2518]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.2532)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2510, -0.1295], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1057, 0.2505], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1057, 0.2505]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.2632)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2497, -0.1351], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1082, 0.2491], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1082, 0.2491]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.2732)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2483, -0.1407], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1107, 0.2477], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1107, 0.2477]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.2832)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2469, -0.1464], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1132, 0.2463], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1132, 0.2463]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.2932)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2454, -0.1521], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1156, 0.2447], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1156, 0.2447]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.3032)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2438, -0.1579], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1181, 0.2432], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1181, 0.2432]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.3132)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2422, -0.1637], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1205, 0.2415], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1205, 0.2415]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.3232)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2405, -0.1695], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1229, 0.2398], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1229, 0.2398]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.3332)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2388, -0.1754], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1253, 0.2381], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1253, 0.2381]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.3432)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2370, -0.1813], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1277, 0.2363], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1277, 0.2363]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.3532)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2352, -0.1872], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1300, 0.2344], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1300, 0.2344]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.3632)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2333, -0.1931], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1323, 0.2325], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1323, 0.2325]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.3732)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2313, -0.1990], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1347, 0.2305], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1347, 0.2305]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.3832)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2293, -0.2049], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1369, 0.2284], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1369, 0.2284]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.3932)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2272, -0.2108], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1392, 0.2263], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1392, 0.2263]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.4032)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2250, -0.2167], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1415, 0.2241], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1415, 0.2241]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.4133)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2228, -0.2226], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1437, 0.2219], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1437, 0.2219]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.4233)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2206, -0.2284], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1459, 0.2196], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1459, 0.2196]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.4333)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2183, -0.2342], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1481, 0.2173], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1481, 0.2173]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.4433)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2159, -0.2399], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1503, 0.2149], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1503, 0.2149]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.4533)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2134, -0.2455], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1524, 0.2124], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1524, 0.2124]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.4633)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2110, -0.2510], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1545, 0.2099], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1545, 0.2099]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.4733)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2084, -0.2565], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1566, 0.2074], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1566, 0.2074]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.4833)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2058, -0.2618], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1586, 0.2047], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1586, 0.2047]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.4933)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2032, -0.2670], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1607, 0.2021], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1607, 0.2021]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.5033)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.2005, -0.2722], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1627, 0.1993], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1627, 0.1993]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.5133)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1977, -0.2772], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1647, 0.1966], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1647, 0.1966]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.5233)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1949, -0.2820], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1666, 0.1937], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1666, 0.1937]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.5333)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1921, -0.2868], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1685, 0.1909], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1685, 0.1909]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.5433)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1892, -0.2914], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1704, 0.1880], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1704, 0.1880]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.5533)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1862, -0.2959], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1723, 0.1850], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1723, 0.1850]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.5633)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1832, -0.3003], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1741, 0.1820], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1741, 0.1820]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.5733)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1802, -0.3046], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1759, 0.1789], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1759, 0.1789]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.5833)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1771, -0.3088], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1777, 0.1759], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1777, 0.1759]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.5933)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1740, -0.3129], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1794, 0.1727], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1794, 0.1727]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.6033)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1709, -0.3169], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1811, 0.1696], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1811, 0.1696]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.6134)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1677, -0.3208], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1828, 0.1663], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1828, 0.1663]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.6234)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1644, -0.3247], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1845, 0.1631], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1845, 0.1631]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.6334)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1612, -0.3285], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1861, 0.1598], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1861, 0.1598]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.6434)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1579, -0.3322], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1877, 0.1565], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1877, 0.1565]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.6534)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1545, -0.3359], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1892, 0.1531], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1892, 0.1531]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.6634)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1511, -0.3396], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1907, 0.1497], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1907, 0.1497]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.6734)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1477, -0.3432], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1922, 0.1463], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1922, 0.1463]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.6834)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1442, -0.3469], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1936, 0.1428], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1936, 0.1428]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.6934)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1408, -0.3505], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1950, 0.1393], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1950, 0.1393]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.7034)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1372, -0.3540], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1964, 0.1358], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1964, 0.1358]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.7134)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1337, -0.3576], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1978, 0.1322], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1978, 0.1322]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.7234)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1301, -0.3611], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.1991, 0.1286], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.1991, 0.1286]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.7334)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1264, -0.3646], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2003, 0.1249], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2003, 0.1249]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.7434)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1228, -0.3680], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2016, 0.1213], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2016, 0.1213]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.7534)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1191, -0.3715], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2027, 0.1175], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2027, 0.1175]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.7634)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1153, -0.3748], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2039, 0.1138], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2039, 0.1138]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.7734)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1115, -0.3781], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2050, 0.1100], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2050, 0.1100]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.7834)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1077, -0.3814], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2061, 0.1062], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2061, 0.1062]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.7934)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1039, -0.3845], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2071, 0.1023], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2071, 0.1023]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.8034)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.1000, -0.3876], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2081, 0.0985], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2081, 0.0985]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.8135)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0961, -0.3905], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2091, 0.0946], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2091, 0.0946]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.8235)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0922, -0.3933], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2100, 0.0906], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2100, 0.0906]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.8335)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0883, -0.3960], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2109, 0.0867], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2109, 0.0867]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.8435)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0843, -0.3986], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2117, 0.0827], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2117, 0.0827]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.8535)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0803, -0.4010], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2125, 0.0787], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2125, 0.0787]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.8635)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0763, -0.4032], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2133, 0.0746], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2133, 0.0746]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.8735)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0722, -0.4053], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2140, 0.0706], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2140, 0.0706]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.8835)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0682, -0.4071], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2147, 0.0665], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2147, 0.0665]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.8935)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0641, -0.4088], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2154, 0.0624], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2154, 0.0624]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.9035)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0600, -0.4103], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2160, 0.0583], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2160, 0.0583]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.9135)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0559, -0.4116], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2165, 0.0542], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2165, 0.0542]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.9235)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0517, -0.4127], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2170, 0.0501], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2170, 0.0501]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.9335)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0476, -0.4136], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2175, 0.0459], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2175, 0.0459]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.9435)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0435, -0.4143], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2179, 0.0418], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2179, 0.0418]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.9535)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0393, -0.4148], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2183, 0.0376], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2183, 0.0376]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.9635)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0352, -0.4151], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2187, 0.0335], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2187, 0.0335]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.9735)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0310, -0.4151], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2190, 0.0293], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2190, 0.0293]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.9835)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0268, -0.4150], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2193, 0.0252], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2193, 0.0252]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(6.9935)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0227, -0.4147], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2195, 0.0210], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2195, 0.0210]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.0036)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0185, -0.4141], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2197, 0.0169], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2197, 0.0169]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.0136)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0144, -0.4134], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2198, 0.0127], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2198, 0.0127]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.0236)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0103, -0.4125], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2199, 0.0086], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2199, 0.0086]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.0336)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0062, -0.4113], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2200, 0.0045], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2200, 0.0045]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.0436)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([ 0.0020, -0.4100], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([1.2200e+00, 3.9181e-04], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[1.2200e+00, 3.9181e-04]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.0536)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0020, -0.4085], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2200, -0.0037], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2200, -0.0037]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.0636)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0061, -0.4067], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2199, -0.0078], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2199, -0.0078]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.0736)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0102, -0.4048], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2198, -0.0118], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2198, -0.0118]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.0836)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0142, -0.4027], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2197, -0.0158], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2197, -0.0158]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.0936)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0182, -0.4004], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2195, -0.0199], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2195, -0.0199]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.1036)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0222, -0.3980], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2193, -0.0238], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2193, -0.0238]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.1136)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0262, -0.3953], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2190, -0.0278], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2190, -0.0278]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.1236)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0301, -0.3925], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2187, -0.0317], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2187, -0.0317]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.1336)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0341, -0.3894], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2184, -0.0356], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2184, -0.0356]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.1436)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0379, -0.3862], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2180, -0.0395], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2180, -0.0395]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.1536)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0418, -0.3829], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2176, -0.0433], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2176, -0.0433]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.1636)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0456, -0.3793], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2171, -0.0471], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2171, -0.0471]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.1736)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0494, -0.3756], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2166, -0.0509], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2166, -0.0509]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.1836)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0531, -0.3718], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2161, -0.0546], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2161, -0.0546]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.1936)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0568, -0.3677], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2155, -0.0583], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2155, -0.0583]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.2037)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0605, -0.3635], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2149, -0.0619], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2149, -0.0619]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.2137)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0641, -0.3592], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2143, -0.0655], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2143, -0.0655]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.2237)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0676, -0.3547], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2136, -0.0690], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2136, -0.0690]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.2337)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0712, -0.3500], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2129, -0.0725], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2129, -0.0725]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.2437)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0746, -0.3452], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2121, -0.0760], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2121, -0.0760]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.2537)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0781, -0.3402], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2114, -0.0794], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2114, -0.0794]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.2637)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0814, -0.3351], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2105, -0.0827], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2105, -0.0827]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.2737)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0848, -0.3298], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2097, -0.0860], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2097, -0.0860]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.2837)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0880, -0.3244], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2088, -0.0893], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2088, -0.0893]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.2937)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0913, -0.3189], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2079, -0.0925], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2079, -0.0925]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.3037)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0944, -0.3132], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2070, -0.0956], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2070, -0.0956]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.3137)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.0975, -0.3074], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2060, -0.0987], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2060, -0.0987]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.3237)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1006, -0.3015], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2050, -0.1017], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2050, -0.1017]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.3337)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1036, -0.2954], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2039, -0.1047], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2039, -0.1047]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.3437)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1065, -0.2892], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2029, -0.1076], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2029, -0.1076]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.3537)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1093, -0.2829], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2018, -0.1104], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2018, -0.1104]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.3637)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1121, -0.2765], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.2007, -0.1132], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.2007, -0.1132]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.3737)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1149, -0.2699], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1995, -0.1159], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1995, -0.1159]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.3837)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1175, -0.2632], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1983, -0.1185], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1983, -0.1185]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.3937)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1201, -0.2565], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1971, -0.1211], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1971, -0.1211]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.4038)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1227, -0.2496], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1959, -0.1236], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1959, -0.1236]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.4138)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1251, -0.2425], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1947, -0.1260], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1947, -0.1260]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.4238)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1275, -0.2354], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1934, -0.1283], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1934, -0.1283]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.4338)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1298, -0.2282], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1921, -0.1306], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1921, -0.1306]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.4438)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1321, -0.2209], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1908, -0.1328], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1908, -0.1328]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.4538)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1343, -0.2135], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1894, -0.1350], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1894, -0.1350]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.4638)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1364, -0.2060], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1881, -0.1370], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1881, -0.1370]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.4738)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1384, -0.1984], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1867, -0.1390], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1867, -0.1390]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.4838)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1403, -0.1907], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1853, -0.1409], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1853, -0.1409]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.4938)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1422, -0.1829], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1838, -0.1427], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1838, -0.1427]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.5038)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1440, -0.1750], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1824, -0.1445], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1824, -0.1445]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.5138)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1457, -0.1671], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1809, -0.1462], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1809, -0.1462]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.5238)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1473, -0.1590], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1795, -0.1478], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1795, -0.1478]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.5338)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1489, -0.1509], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1780, -0.1493], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1780, -0.1493]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.5438)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1504, -0.1427], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1765, -0.1507], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1765, -0.1507]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.5538)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1517, -0.1345], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1750, -0.1520], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1750, -0.1520]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.5638)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1530, -0.1261], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1734, -0.1533], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1734, -0.1533]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.5738)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1543, -0.1177], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1719, -0.1545], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1719, -0.1545]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.5838)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1554, -0.1093], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1703, -0.1556], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1703, -0.1556]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.5938)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1565, -0.1007], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1688, -0.1566], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1688, -0.1566]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.6039)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1574, -0.0921], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1672, -0.1575], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1672, -0.1575]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.6139)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1583, -0.0835], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1656, -0.1583], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1656, -0.1583]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.6239)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1591, -0.0748], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1640, -0.1591], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1640, -0.1591]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.6339)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1598, -0.0660], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1624, -0.1598], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1624, -0.1598]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.6439)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1604, -0.0572], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1608, -0.1603], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1608, -0.1603]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.6539)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1609, -0.0483], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1592, -0.1608], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1592, -0.1608]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.6639)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1614, -0.0394], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1576, -0.1612], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1576, -0.1612]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.6739)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1617, -0.0304], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1560, -0.1615], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1560, -0.1615]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.6839)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1620, -0.0214], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1543, -0.1617], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1543, -0.1617]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.6939)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1622, -0.0124], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1527, -0.1618], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1527, -0.1618]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.7039)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1622, -0.0033], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1511, -0.1619], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1511, -0.1619]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.7139)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1622,  0.0059], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1495, -0.1618], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1495, -0.1618]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.7239)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1621,  0.0150], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1479, -0.1617], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1479, -0.1617]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.7339)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1619,  0.0242], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1462, -0.1614], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1462, -0.1614]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.7439)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1616,  0.0334], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1446, -0.1611], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1446, -0.1611]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.7539)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1613,  0.0427], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1430, -0.1607], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1430, -0.1607]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.7639)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1608,  0.0520], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1414, -0.1601], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1414, -0.1601]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.7739)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1602,  0.0613], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1398, -0.1595], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1398, -0.1595]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.7839)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1596,  0.0706], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1382, -0.1588], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1382, -0.1588]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.7940)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1588,  0.0800], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1366, -0.1580], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1366, -0.1580]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.8040)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1580,  0.0894], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1350, -0.1571], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1350, -0.1571]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.8140)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1570,  0.0988], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1335, -0.1561], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1335, -0.1561]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.8240)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1560,  0.1082], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1319, -0.1551], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1319, -0.1551]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.8340)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1549,  0.1176], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1303, -0.1539], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[ 1.1303, -0.1539]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(7.8440)\n",
      "dt:  tensor(0.0100)\n",
      "dudt[i,:]:  tensor([-0.1536,  0.1270], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([ 1.1288, -0.1526], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmax = 2.5*np.pi\n",
    "nsamples = 100*tmax\n",
    "dt = tmax/nsamples\n",
    "\n",
    "nt = round(tmax / dt)\n",
    "t_sample = np.linspace(0, tmax, nt + 1)\n",
    "integrator = \"midpoint\"\n",
    "ntrajectories = 1\n",
    "\n",
    "(u_start, u_end, t_start, dt), dudt, u_exact, u0s =  generate_data(system=sys,ntrajectories =ntrajectories, t_sample = t_sample,integrator=integrator)\n",
    "\n",
    "u_phnn_exp, t_sample = model_exp.generate_trajectories(ntrajectories = ntrajectories, t_sample = t_sample,integrator = integrator,u0s=u0s)\n",
    "\n",
    "u_pred = u_phnn_exp[0].detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))  \n",
    "y, py = u_exact[0][:, 0], u_exact[0][:, 1]\n",
    "ax.plot(y, py, label=\"Exact\")\n",
    "y, py = u_pred[:, 0], u_pred[:, 1]\n",
    "ax.plot(y, py, label=\"PHNN\")\n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(\"py\")\n",
    "ax.set_title(\"Phase Space Trajectory Explicit Midpoint\" )\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3c344",
   "metadata": {},
   "source": [
    "## Symplectic Midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e64d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1750/1750 [01:17<00:00, 22.55it/s]\n",
      "100%|██████████| 750/750 [00:32<00:00, 23.09it/s]\n"
     ]
    }
   ],
   "source": [
    "integrator = \"symplectic midpoint\"\n",
    "\n",
    "(u_start, u_end, t_start, dt), dudt, u_train, u0s_train  =  generate_data(system=sys,ntrajectories =ntraj_train, t_sample = t_train,integrator=integrator, u0s = u0s_train)\n",
    "train_data = (u_start, u_end,t_start, dt), dudt\n",
    "\n",
    "(u_start, u_end,t_start, dt), dudt, u_val, u0s_val =  generate_data(system=sys,ntrajectories =ntraj_val, t_sample = t_val,integrator=integrator, u0s = u0s_val)\n",
    "val_data = (u_start, u_end, t_start, dt), dudt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eebfa317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:46:09<00:00, 127.39s/it, epoch=49, loss=4.63e-5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Hamiltonian_est = BaseHamiltonianNeuralNetwork(nstates = 2, act_1 = act_1, act_2 = act_2, act_3 = act_3)\n",
    "External_Forces_est = ExternalForceNeuralNetwork(nstates = 2, act_1 = act_1, act_2 = act_2, act_3 = act_3)\n",
    "\n",
    "\n",
    "model_symp = PortHamiltonianNeuralNetwork(nstates = 2, S = sys.S, Hamiltonian_est = Hamiltonian_est, External_Forces_est = External_Forces_est)\n",
    "\n",
    "optimizer_symp = torch.optim.Adam(model_symp.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "trainer = Training(model = model_symp,integrator = integrator, train_data = train_data, val_data = val_data,optimizer = optimizer_symp, system = sys, batch_size=batch_size,epochs = epochs)\n",
    "model_symp, trainingdetails_symp = trainer.train(loss_func=loss_func,penalty_func = penalty_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5fd63013",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[-0.2084,  0.5009]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fc1cb63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.10420667838547841)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.Hamiltonian(test,t=1.1499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "36fd16b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19934907],\n",
       "       [0.5009    ]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.Hamiltonian_grad(test,t=1.1499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "71be8429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5009    , -0.15325292]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.u_dot(test,np.array([1.1499]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "efd63a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1745]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_symp.Hamiltonian(torch.tensor(test,dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "99375bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1758, 0.5007]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_symp.dH(torch.tensor(test,dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "24cdda1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5007, -0.1541]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_symp.u_dot(torch.tensor(test,dtype=torch.float32),torch.tensor([1.1499],dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f791eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_torch(func, guess, threshold=1e-7, max_iters=100, damping=1.0):\n",
    "    guess = torch.tensor(guess, dtype=torch.float32, requires_grad=True)\n",
    "    for i in range(max_iters):\n",
    "        value = func(guess) \n",
    "        if torch.linalg.norm(value) < threshold:\n",
    "            print(\"Jippi.\")\n",
    "            return guess\n",
    "        J = torch.autograd.functional.jacobian(func, guess)  \n",
    "        try:\n",
    "            step = torch.linalg.solve(J, -value)  \n",
    "            print(\"Jippi.\")\n",
    "        except RuntimeError:\n",
    "            print(\"Jacobian is singular, stopping.\")\n",
    "            return guess\n",
    "        guess = guess + damping * step \n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e0489b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_torch(func, guess, threshold=1e-7, max_iters=100, damping=1.0):\n",
    "    guess = torch.tensor(guess, dtype=torch.float32, requires_grad=True)\n",
    "    for i in range(max_iters):\n",
    "        value = func(guess) \n",
    "        if torch.linalg.norm(value) < threshold:\n",
    "            print(\"Jippi.\")\n",
    "            return guess\n",
    "        J = torch.autograd.functional.jacobian(func, guess)  \n",
    "        try:\n",
    "            step = torch.linalg.solve(J, -value)  \n",
    "            print(\"Jippi.\")\n",
    "        except RuntimeError:\n",
    "            print(\"Jacobian is singular, stopping.\")\n",
    "            return guess\n",
    "        guess = guess + damping * step \n",
    "    return guess\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def symplectic_midpoint_time_derivative_withtime(u_dot,u_start, t_start, dt,u_end = None):\n",
    "    t_end = t_start + dt/2\n",
    "    if u_end == None:\n",
    "        def g(u):\n",
    "            return u-u_start-dt*u_dot(0.5*(u+u_start),t_end)\n",
    "        if isinstance(u_start,torch.Tensor):\n",
    "            original_shape = u_start.shape\n",
    "            u_start = u_start.squeeze(0) \n",
    "            u_end = newton_torch(g,u_start)\n",
    "            u_mid = 0.5*(u_start + u_end)\n",
    "            return u_dot(u_mid.view(original_shape), t_end)\n",
    "        else:\n",
    "            u_end = newton(g,u_start, tol = 1e-7, maxiter = 1000)\n",
    "    u_mid = 0.5*(u_start + u_end)\n",
    "    lhs = u_dot(u_mid, t_end)\n",
    "    return lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "330c9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sim_traj(model,t_sample,dt,u0=None):\n",
    "        if u0 is None:\n",
    "            u0 = model.initial_condition_sampler()\n",
    " \n",
    "        u0 = torch.tensor(u0,dtype = torch.float32)\n",
    "        u0 = u0.reshape(1,u0.shape[-1])\n",
    "\n",
    "        t_sample = torch.tensor(t_sample,dtype = torch.float32)\n",
    "        #t_shape = t_sample.shape[-1]\n",
    "\n",
    "        #Initializing solution \n",
    "        u = torch.zeros([t_sample.shape[-1],model.nstates])\n",
    "        dudt = torch.zeros_like(u)\n",
    "\n",
    "        #Setting initial conditions\n",
    "        u[0, :] = u0\n",
    "\n",
    "        #for i in range(t_shape-1):\n",
    "        for i, t_step in enumerate(t_sample[:-1]):\n",
    "            dt = t_sample[i + 1] - t_step\n",
    "          \n",
    "            dudt[i,:] = symplectic_midpoint_time_derivative_withtime(u_dot = model.u_dot, u_start = u[i : i + 1, :], dt=dt,u_end = None, t_start = t_step )\n",
    "\n",
    "\n",
    "            u[i+1,:] = u[i,:] + dt*dudt[i,:]\n",
    "     \n",
    "        return u,dudt,u0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d92fdf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/4gdmfnt15kl39xg3l41t9hr00000gn/T/ipykernel_68052/2245103375.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  u0 = torch.tensor(u0,dtype = torch.float32)\n",
      "/var/folders/hz/4gdmfnt15kl39xg3l41t9hr00000gn/T/ipykernel_68052/1480690111.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  guess = torch.tensor(guess, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m integrator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymplectic midpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m ntrajectories \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mtest_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_symp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mntrajectories\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mntrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu0s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu0s\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[99], line 49\u001b[0m, in \u001b[0;36mtest_generation\u001b[0;34m(model, ntrajectories, t_sample, u0s)\u001b[0m\n\u001b[1;32m     46\u001b[0m u[:,\u001b[38;5;241m0\u001b[39m,:] \u001b[38;5;241m=\u001b[39m u0s\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ntrajectories):\n\u001b[0;32m---> 49\u001b[0m     u[i] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_sim_traj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu0s\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u, t_sample\n",
      "Cell \u001b[0;32mIn[99], line 22\u001b[0m, in \u001b[0;36mtest_sim_traj\u001b[0;34m(model, t_sample, dt, u0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, t_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(t_sample[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     20\u001b[0m     dt \u001b[38;5;241m=\u001b[39m t_sample[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m t_step\n\u001b[0;32m---> 22\u001b[0m     dudt[i,:] \u001b[38;5;241m=\u001b[39m \u001b[43msymplectic_midpoint_time_derivative_withtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_dot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu_dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_step\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     u[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;241m=\u001b[39m u[i,:] \u001b[38;5;241m+\u001b[39m dt\u001b[38;5;241m*\u001b[39mdudt[i,:]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u,dudt,u0\n",
      "Cell \u001b[0;32mIn[97], line 31\u001b[0m, in \u001b[0;36msymplectic_midpoint_time_derivative_withtime\u001b[0;34m(u_dot, u_start, t_start, dt, u_end)\u001b[0m\n\u001b[1;32m     29\u001b[0m original_shape \u001b[38;5;241m=\u001b[39m u_start\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     30\u001b[0m u_start \u001b[38;5;241m=\u001b[39m u_start\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m---> 31\u001b[0m u_end \u001b[38;5;241m=\u001b[39m \u001b[43mnewton_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu_start\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m u_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(u_start \u001b[38;5;241m+\u001b[39m u_end)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u_dot(u_mid\u001b[38;5;241m.\u001b[39mview(original_shape), t_end)\n",
      "Cell \u001b[0;32mIn[97], line 4\u001b[0m, in \u001b[0;36mnewton_torch\u001b[0;34m(func, guess, threshold, max_iters, damping)\u001b[0m\n\u001b[1;32m      2\u001b[0m guess \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(guess, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m----> 4\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mguess\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(value) \u001b[38;5;241m<\u001b[39m threshold:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJippi.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[97], line 27\u001b[0m, in \u001b[0;36msymplectic_midpoint_time_derivative_withtime.<locals>.g\u001b[0;34m(u)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mg\u001b[39m(u):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m u\u001b[38;5;241m-\u001b[39mu_start\u001b[38;5;241m-\u001b[39mdt\u001b[38;5;241m*\u001b[39m\u001b[43mu_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mu_start\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt_end\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/HamiltonianNeuralNetwork/pHNN.py:146\u001b[0m, in \u001b[0;36mPortHamiltonianNeuralNetwork.u_dot\u001b[0;34m(self, u, t_start)\u001b[0m\n\u001b[1;32m    143\u001b[0m qdot \u001b[38;5;241m=\u001b[39m u_dot[:,:\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnstates\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)]\n\u001b[1;32m    144\u001b[0m pdot \u001b[38;5;241m=\u001b[39m u_dot[:,\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnstates\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m):]\n\u001b[0;32m--> 146\u001b[0m F \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExternal_Force\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m new_pdot \u001b[38;5;241m=\u001b[39m pdot \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN\u001b[38;5;241m*\u001b[39mqdot \u001b[38;5;241m+\u001b[39m F\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([qdot, new_pdot], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/HamiltonianNeuralNetwork/pHNN.py:132\u001b[0m, in \u001b[0;36mPortHamiltonianNeuralNetwork.External_Force\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mExternal_Force\u001b[39m(\u001b[38;5;28mself\u001b[39m,t):\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExternal_Forces_est\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/HamiltonianNeuralNetwork/pHNN.py:73\u001b[0m, in \u001b[0;36mExternalForceNeuralNetwork.forward\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,t\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/HamiltonianNeuralNetwork/HNN.py:23\u001b[0m, in \u001b[0;36mPAU.forward\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, u):\n\u001b[0;32m---> 23\u001b[0m     num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerator\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_numerator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     den \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenominator[i] \u001b[38;5;241m*\u001b[39m u\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_denominator)))\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m num \u001b[38;5;241m/\u001b[39m den\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/HamiltonianNeuralNetwork/HNN.py:23\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, u):\n\u001b[0;32m---> 23\u001b[0m     num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumerator[i] \u001b[38;5;241m*\u001b[39m u\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_numerator \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     24\u001b[0m     den \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenominator[i] \u001b[38;5;241m*\u001b[39m u\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_denominator)))\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m num \u001b[38;5;241m/\u001b[39m den\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_sim_traj(model,t_sample,dt,u0=None):\n",
    "        if u0 is None:\n",
    "            u0 = model.initial_condition_sampler()\n",
    " \n",
    "        u0 = torch.tensor(u0,dtype = torch.float32)\n",
    "        u0 = u0.reshape(1,u0.shape[-1])\n",
    "\n",
    "        t_sample = torch.tensor(t_sample,dtype = torch.float32)\n",
    "        #t_shape = t_sample.shape[-1]\n",
    "\n",
    "        #Initializing solution \n",
    "        u = torch.zeros([t_sample.shape[-1],model.nstates])\n",
    "        dudt = torch.zeros_like(u)\n",
    "\n",
    "        #Setting initial conditions\n",
    "        u[0, :] = u0\n",
    "\n",
    "        #for i in range(t_shape-1):\n",
    "        for i, t_step in enumerate(t_sample[:-1]):\n",
    "            dt = t_sample[i + 1] - t_step\n",
    "          \n",
    "            dudt[i,:] = symplectic_midpoint_time_derivative_withtime(u_dot = model.u_dot, u_start = u[i : i + 1, :], dt=dt,u_end = None, t_start = t_step )\n",
    "\n",
    "\n",
    "            u[i+1,:] = u[i,:] + dt*dudt[i,:]\n",
    "     \n",
    "        return u,dudt,u0\n",
    "\n",
    "def test_generation(model,ntrajectories, t_sample,u0s=None):\n",
    "        if u0s is None:\n",
    "            u0s = model.initial_condition_sampler(ntrajectories)\n",
    "        \n",
    "        #Reshaping\n",
    "        u0s = torch.tensor(u0s,dtype = torch.float32)\n",
    "        u0s = u0s.reshape(ntrajectories, model.nstates)\n",
    "        t_sample = torch.tensor(t_sample,dtype = torch.float32)\n",
    "        if len(t_sample.shape) == 1:\n",
    "                #Reshaping time\n",
    "                t_sample = np.tile(t_sample, (ntrajectories, 1))\n",
    "\n",
    "        dt = t_sample[0, 1] - t_sample[0, 0]\n",
    "        traj_length = t_sample.shape[-1]\n",
    "\n",
    "        #Initializng u and setting initial conditions\n",
    "        u = torch.zeros([ntrajectories, traj_length, model.nstates])\n",
    "        u[:,0,:] = u0s\n",
    "\n",
    "        for i in range(ntrajectories):\n",
    "            u[i] = test_sim_traj(model = model,t_sample = t_sample[i], u0 = u0s[i],dt=dt)[0]\n",
    "            \n",
    "   \n",
    "        return u, t_sample\n",
    "tmax = 2*np.pi\n",
    "nsamples = 1000*tmax\n",
    "dt = tmax/nsamples\n",
    "\n",
    "nt = round(tmax / dt)\n",
    "t_sample = np.linspace(0, tmax, nt + 1)\n",
    "integrator = \"symplectic midpoint\"\n",
    "ntrajectories = 1\n",
    "\n",
    "test_generation(model = model_symp,ntrajectories = ntrajectories, t_sample = t_sample,u0s=u0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e61d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/4gdmfnt15kl39xg3l41t9hr00000gn/T/ipykernel_68052/1304074440.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  u0 = torch.tensor(u0,dtype = torch.float32)\n",
      "/var/folders/hz/4gdmfnt15kl39xg3l41t9hr00000gn/T/ipykernel_68052/2092351003.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  guess = torch.tensor(guess, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n",
      "Jippi.\n",
      "Jacobian is singular, stopping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_symp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mntrajectories\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mntrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu0s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu0s\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[76], line 21\u001b[0m, in \u001b[0;36mtest_generation\u001b[0;34m(model, ntrajectories, t_sample, u0s)\u001b[0m\n\u001b[1;32m     18\u001b[0m u[:,\u001b[38;5;241m0\u001b[39m,:] \u001b[38;5;241m=\u001b[39m u0s\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ntrajectories):\n\u001b[0;32m---> 21\u001b[0m     u[i] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_sim_traj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu0s\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u, t_sample\n",
      "Cell \u001b[0;32mIn[75], line 22\u001b[0m, in \u001b[0;36mtest_sim_traj\u001b[0;34m(model, t_sample, dt, u0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, t_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(t_sample[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     20\u001b[0m     dt \u001b[38;5;241m=\u001b[39m t_sample[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m t_step\n\u001b[0;32m---> 22\u001b[0m     dudt[i,:] \u001b[38;5;241m=\u001b[39m \u001b[43msymplectic_midpoint_time_derivative_withtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_dot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu_dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_step\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     u[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;241m=\u001b[39m u[i,:] \u001b[38;5;241m+\u001b[39m dt\u001b[38;5;241m*\u001b[39mdudt[i,:]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u,dudt,u0\n",
      "Cell \u001b[0;32mIn[74], line 9\u001b[0m, in \u001b[0;36msymplectic_midpoint_time_derivative_withtime\u001b[0;34m(u_dot, u_start, t_start, dt, u_end)\u001b[0m\n\u001b[1;32m      7\u001b[0m original_shape \u001b[38;5;241m=\u001b[39m u_start\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      8\u001b[0m u_start \u001b[38;5;241m=\u001b[39m u_start\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m----> 9\u001b[0m u_end \u001b[38;5;241m=\u001b[39m \u001b[43mnewton_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu_start\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m u_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(u_start \u001b[38;5;241m+\u001b[39m u_end)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u_dot(u_mid\u001b[38;5;241m.\u001b[39mview(original_shape), t_end)\n",
      "Cell \u001b[0;32mIn[80], line 8\u001b[0m, in \u001b[0;36mnewton_torch\u001b[0;34m(func, guess, threshold, max_iters, damping)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJippi.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guess\n\u001b[0;32m----> 8\u001b[0m J \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguess\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     step \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(J, \u001b[38;5;241m-\u001b[39mvalue)  \n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/autograd/functional.py:789\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    787\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 789\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)\n\u001b[1;32m    798\u001b[0m     ):\n\u001b[1;32m    799\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/autograd/functional.py:195\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_generation(model = model_symp,ntrajectories = ntrajectories, t_sample = t_sample,u0s=u0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09333b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_start = u[i : i + 1, :]:  tensor([[-1.0106,  0.9424]])\n",
      "t_start = t_step:  tensor(0.)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9417, -0.2701], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-1.0012,  0.9397], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-1.0012,  0.9397]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0100)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9390, -0.2859], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.9918,  0.9368], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.9918,  0.9368]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0200)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9363, -0.3003], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.9824,  0.9338], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.9824,  0.9338]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0300)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9334, -0.3143], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.9731,  0.9306], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.9731,  0.9306]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0400)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9303, -0.3280], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.9638,  0.9274], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.9638,  0.9274]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0500)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9270, -0.3412], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.9545,  0.9240], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.9545,  0.9240]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0600)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9236, -0.3537], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.9453,  0.9204], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.9453,  0.9204]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0700)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9190, -0.3660], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.9361,  0.9168], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.9361,  0.9168]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0800)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9154, -0.3774], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.9269,  0.9130], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.9269,  0.9130]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.0900)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9116, -0.3883], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.9178,  0.9091], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.9178,  0.9091]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1000)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9077, -0.3987], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.9087,  0.9051], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.9087,  0.9051]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1100)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.9038, -0.4085], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8997,  0.9010], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8997,  0.9010]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1200)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8997, -0.4178], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8907,  0.8969], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8907,  0.8969]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1300)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8955, -0.4266], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8817,  0.8926], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8817,  0.8926]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1400)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8912, -0.4348], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8728,  0.8882], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8728,  0.8882]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1500)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8869, -0.4427], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8640,  0.8838], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8640,  0.8838]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1600)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8822, -0.4503], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8551,  0.8793], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8551,  0.8793]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1700)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8777, -0.4570], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8464,  0.8747], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8464,  0.8747]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1800)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8732, -0.4632], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8376,  0.8701], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8376,  0.8701]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.1900)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8685, -0.4690], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8290,  0.8654], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8290,  0.8654]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2000)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8639, -0.4744], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8203,  0.8607], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8203,  0.8607]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2100)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8592, -0.4794], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8117,  0.8559], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8117,  0.8559]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2200)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8544, -0.4839], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.8032,  0.8510], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.8032,  0.8510]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2300)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8496, -0.4881], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7947,  0.8462], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7947,  0.8462]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2400)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8447, -0.4918], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7862,  0.8413], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7862,  0.8413]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2500)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8398, -0.4952], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7778,  0.8363], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7778,  0.8363]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2600)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8348, -0.4982], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7695,  0.8313], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7695,  0.8313]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2700)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8299, -0.5009], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7612,  0.8263], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7612,  0.8263]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2800)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8249, -0.5032], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7530,  0.8213], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7530,  0.8213]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.2900)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8199, -0.5051], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7448,  0.8162], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7448,  0.8162]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3000)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8148, -0.5068], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7366,  0.8112], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7366,  0.8112]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3100)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8098, -0.5081], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7285,  0.8061], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7285,  0.8061]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3200)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.8047, -0.5092], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7205,  0.8010], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7205,  0.8010]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3300)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7996, -0.5099], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7125,  0.7959], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7125,  0.7959]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3400)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7945, -0.5104], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.7045,  0.7908], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.7045,  0.7908]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3500)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7894, -0.5107], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6966,  0.7857], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6966,  0.7857]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3600)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7843, -0.5105], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6888,  0.7806], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6888,  0.7806]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3700)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7792, -0.5100], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6810,  0.7755], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6810,  0.7755]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3800)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7741, -0.5093], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6733,  0.7704], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6733,  0.7704]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3899)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7690, -0.5084], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6656,  0.7653], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6656,  0.7653]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.3999)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7636, -0.5071], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6579,  0.7602], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6579,  0.7602]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4099)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7585, -0.5057], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6504,  0.7552], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6504,  0.7552]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4199)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7534, -0.5041], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6428,  0.7501], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6428,  0.7501]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4299)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7483, -0.5023], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6353,  0.7451], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6353,  0.7451]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4399)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7433, -0.5004], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6279,  0.7401], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6279,  0.7401]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4499)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7382, -0.4982], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6205,  0.7351], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6205,  0.7351]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4599)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7332, -0.4959], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6132,  0.7302], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6132,  0.7302]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4699)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7282, -0.4934], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.6059,  0.7252], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.6059,  0.7252]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4799)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7233, -0.4907], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5987,  0.7203], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5987,  0.7203]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4899)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7183, -0.4879], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5915,  0.7155], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5915,  0.7155]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.4999)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7134, -0.4849], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5844,  0.7106], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5844,  0.7106]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5099)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7086, -0.4819], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5773,  0.7058], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5773,  0.7058]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5199)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.7037, -0.4786], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5702,  0.7010], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5702,  0.7010]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5299)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6989, -0.4753], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5633,  0.6962], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5633,  0.6962]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5399)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6941, -0.4719], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5563,  0.6915], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5563,  0.6915]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5499)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6894, -0.4682], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5494,  0.6868], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5494,  0.6868]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5599)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6847, -0.4643], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5426,  0.6822], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5426,  0.6822]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5699)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6800, -0.4589], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5358,  0.6776], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5358,  0.6776]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5799)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6755, -0.4548], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5290,  0.6731], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5290,  0.6731]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5899)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6709, -0.4506], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5223,  0.6686], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5223,  0.6686]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.5999)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6665, -0.4466], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5156,  0.6641], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5156,  0.6641]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6099)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6620, -0.4422], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5090,  0.6597], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5090,  0.6597]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6199)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6576, -0.4377], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.5025,  0.6553], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.5025,  0.6553]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6299)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6532, -0.4332], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4959,  0.6510], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4959,  0.6510]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6399)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6489, -0.4286], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4894,  0.6467], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4894,  0.6467]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6499)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6446, -0.4239], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4830,  0.6424], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4830,  0.6424]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6599)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6404, -0.4192], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4766,  0.6383], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4766,  0.6383]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6699)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6362, -0.4144], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4702,  0.6341], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4702,  0.6341]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6799)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6320, -0.4095], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4639,  0.6300], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4639,  0.6300]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6899)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6279, -0.4047], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4576,  0.6260], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4576,  0.6260]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.6999)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6239, -0.3997], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4514,  0.6220], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4514,  0.6220]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7099)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6199, -0.3947], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4452,  0.6180], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4452,  0.6180]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7199)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6160, -0.3897], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4390,  0.6141], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4390,  0.6141]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7299)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6121, -0.3835], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4329,  0.6103], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4329,  0.6103]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7399)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6084, -0.3774], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4268,  0.6065], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4268,  0.6065]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7499)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6045, -0.3734], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4208,  0.6028], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4208,  0.6028]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7599)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.6008, -0.3679], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4148,  0.5991], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4148,  0.5991]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7699)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5972, -0.3623], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4088,  0.5955], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4088,  0.5955]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7799)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5936, -0.3567], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.4029,  0.5919], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.4029,  0.5919]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7899)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5900, -0.3511], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3970,  0.5884], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3970,  0.5884]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.7999)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5865, -0.3455], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3911,  0.5850], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3911,  0.5850]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8099)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5831, -0.3399], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3853,  0.5816], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3853,  0.5816]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8199)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5797, -0.3342], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3795,  0.5782], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3795,  0.5782]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8299)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5764, -0.3286], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3737,  0.5749], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3737,  0.5749]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8399)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5732, -0.3229], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3680,  0.5717], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3680,  0.5717]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8499)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5700, -0.3173], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3623,  0.5685], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3623,  0.5685]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8599)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5668, -0.3115], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3566,  0.5654], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3566,  0.5654]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8699)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5637, -0.3057], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3510,  0.5624], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3510,  0.5624]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8799)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5607, -0.3001], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3454,  0.5594], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3454,  0.5594]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8899)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5577, -0.2947], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3398,  0.5564], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3398,  0.5564]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.8999)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5548, -0.2891], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3343,  0.5535], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3343,  0.5535]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9099)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5519, -0.2835], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3287,  0.5507], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3287,  0.5507]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9199)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5491, -0.2778], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3232,  0.5479], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3232,  0.5479]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9299)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5464, -0.2721], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3178,  0.5452], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3178,  0.5452]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9399)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5437, -0.2665], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3123,  0.5425], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3123,  0.5425]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9499)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5410, -0.2608], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3069,  0.5399], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3069,  0.5399]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9599)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5384, -0.2552], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.3016,  0.5374], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.3016,  0.5374]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9699)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5359, -0.2496], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2962,  0.5349], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2962,  0.5349]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9799)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5335, -0.2441], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2909,  0.5324], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2909,  0.5324]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9899)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5310, -0.2385], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2855,  0.5300], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2855,  0.5300]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(0.9999)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5287, -0.2329], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2803,  0.5277], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2803,  0.5277]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0099)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5264, -0.2272], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2750,  0.5254], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2750,  0.5254]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0199)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5241, -0.2216], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2698,  0.5232], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2698,  0.5232]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0299)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5219, -0.2162], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2645,  0.5211], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2645,  0.5211]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0399)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5198, -0.2108], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2593,  0.5190], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2593,  0.5190]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0499)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5177, -0.2053], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2542,  0.5169], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2542,  0.5169]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0599)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5157, -0.1997], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2490,  0.5149], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2490,  0.5149]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0699)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5137, -0.1942], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2439,  0.5130], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2439,  0.5130]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0799)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5118, -0.1887], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2388,  0.5111], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2388,  0.5111]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0899)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5100, -0.1833], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2337,  0.5092], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2337,  0.5092]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.0999)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5082, -0.1779], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2286,  0.5075], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2286,  0.5075]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1099)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5064, -0.1727], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2235,  0.5057], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2235,  0.5057]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1199)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5047, -0.1675], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2185,  0.5041], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2185,  0.5041]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1299)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5031, -0.1623], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2134,  0.5024], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2134,  0.5024]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1399)\n",
      "dt:  tensor(0.0100)\n",
      "Jacobian is singular, stopping.\n",
      "dudt[i,:]:  tensor([ 0.5015, -0.1570], grad_fn=<SliceBackward0>)\n",
      "u[i+1,:]:  tensor([-0.2084,  0.5009], grad_fn=<SliceBackward0>)\n",
      "u_start = u[i : i + 1, :]:  tensor([[-0.2084,  0.5009]], grad_fn=<SliceBackward0>)\n",
      "t_start = t_step:  tensor(1.1499)\n",
      "dt:  tensor(0.0100)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m ntrajectories \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     10\u001b[0m (u_start, u_end, t_start, dt), dudt, u_exact, u0s \u001b[38;5;241m=\u001b[39m  generate_data(system\u001b[38;5;241m=\u001b[39msys,ntrajectories \u001b[38;5;241m=\u001b[39mntrajectories, t_sample \u001b[38;5;241m=\u001b[39m t_sample,integrator\u001b[38;5;241m=\u001b[39mintegrator)\n\u001b[0;32m---> 12\u001b[0m u_phnn_symp, t_sample \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_symp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mntrajectories\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mntrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43mintegrator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mintegrator\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu0s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m u_pred \u001b[38;5;241m=\u001b[39m u_phnn_symp[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     16\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m5\u001b[39m))  \n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/HamiltonianNeuralNetwork/pHNN.py:202\u001b[0m, in \u001b[0;36mPortHamiltonianNeuralNetwork.generate_trajectories\u001b[0;34m(self, ntrajectories, t_sample, integrator, u0s)\u001b[0m\n\u001b[1;32m    199\u001b[0m u[:,\u001b[38;5;241m0\u001b[39m,:] \u001b[38;5;241m=\u001b[39m u0s\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ntrajectories):\n\u001b[0;32m--> 202\u001b[0m     u[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintegrator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mintegrator\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu0s\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u, t_sample\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/HamiltonianNeuralNetwork/pHNN.py:176\u001b[0m, in \u001b[0;36mPortHamiltonianNeuralNetwork.simulate_trajectory\u001b[0;34m(self, integrator, t_sample, dt, u0)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt_start = t_step: \u001b[39m\u001b[38;5;124m\"\u001b[39m, t_step)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdt: \u001b[39m\u001b[38;5;124m\"\u001b[39m, dt)\n\u001b[0;32m--> 176\u001b[0m dudt[i,:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_derivative_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintegrator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintegrator\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdudt[i,:]: \u001b[39m\u001b[38;5;124m\"\u001b[39m, dudt[i,:])\n\u001b[1;32m    178\u001b[0m u[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;241m=\u001b[39m u[i,:] \u001b[38;5;241m+\u001b[39m dt\u001b[38;5;241m*\u001b[39mdudt[i,:]\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/HamiltonianNeuralNetwork/pHNN.py:103\u001b[0m, in \u001b[0;36mPortHamiltonianNeuralNetwork.time_derivative_step\u001b[0;34m(self, integrator, u_start, dt, u_end, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m explicit_midpoint_time_derivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_dot, u_start, dt, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m integrator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymplectic midpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msymplectic_midpoint_time_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu_dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m integrator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymplectic euler\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m symplectic_euler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_dot, u_start, dt)\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/NumericalIntegration/Numerical_Integration.py:106\u001b[0m, in \u001b[0;36msymplectic_midpoint_time_derivative\u001b[0;34m(u_dot, u_start, dt, u_end, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m original_shape \u001b[38;5;241m=\u001b[39m u_start\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    105\u001b[0m u_start \u001b[38;5;241m=\u001b[39m u_start\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m u_end \u001b[38;5;241m=\u001b[39m \u001b[43mnewton_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_start\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m u_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (u_start \u001b[38;5;241m+\u001b[39m u_end)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u_dot(u_mid\u001b[38;5;241m.\u001b[39mview(original_shape), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/NumericalIntegration/Numerical_Integration.py:12\u001b[0m, in \u001b[0;36mnewton_torch\u001b[0;34m(func, guess, threshold, max_iters, damping)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(value) \u001b[38;5;241m<\u001b[39m threshold:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guess\n\u001b[0;32m---> 12\u001b[0m J \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguess\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     step \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(J, \u001b[38;5;241m-\u001b[39mvalue)  \n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/autograd/functional.py:789\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    787\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 789\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)\n\u001b[1;32m    798\u001b[0m     ):\n\u001b[1;32m    799\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/autograd/functional.py:195\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/Documents/Vår 2025/Prosjektoppgave/Oppgave_github/HNNs_ODEs/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tmax = 10*np.pi\n",
    "nsamples = 100*tmax\n",
    "dt = tmax/nsamples\n",
    "\n",
    "nt = round(tmax / dt)\n",
    "t_sample = np.linspace(0, tmax, nt + 1)\n",
    "integrator = \"symplectic midpoint\"\n",
    "ntrajectories = 1\n",
    "\n",
    "(u_start, u_end, t_start, dt), dudt, u_exact, u0s =  generate_data(system=sys,ntrajectories =ntrajectories, t_sample = t_sample,integrator=integrator)\n",
    "\n",
    "u_phnn_symp, t_sample = model_symp.generate_trajectories(ntrajectories = ntrajectories, t_sample = t_sample,integrator = integrator,u0s=u0s)\n",
    "\n",
    "u_pred = u_phnn_symp[0].detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))  \n",
    "y, py = u_exact[0][:, 0], u_exact[0][:, 1]\n",
    "ax.plot(y, py, label=\"Exact\")\n",
    "y, py = u_pred[:, 0], u_pred[:, 1]\n",
    "ax.plot(y, py, label=\"PHNN\")\n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(\"py\")\n",
    "ax.set_title(\"Phase Space Trajectory Explicit Midpoint\" )\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
