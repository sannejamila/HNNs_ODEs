{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from HamiltonianNeuralNetwork.Train import *\n",
    "from Systems.DuffingSystem import *\n",
    "from Systems.Generate_Data import *\n",
    "from HamiltonianNeuralNetwork.pHNN import *\n",
    "from NumericalIntegration.Numerical_Integration import *\n",
    "\n",
    "torch.random.manual_seed(1)\n",
    "np.random.seed(33)\n",
    "#torch.set_default_device(\"mps\")\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['lines.markersize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "colors = sns.color_palette([(0.6,0.8,.8), (1,0.7,0.3), (0.2,0.7,0.2), (0.8,0,0.2), (0,0.4,1), (0.6,0.5,.9), (0.5,0.3,.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52bbc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_condition():\n",
    "    def sampler():\n",
    "        u0 =  np.random.rand(2) * 2 - 1\n",
    "        radius = np.sqrt(np.random.uniform(0.5, 1.5))  \n",
    "        u0 /= np.sqrt((u0 ** 2).sum()) * (radius)\n",
    "        #u0 = np.random.rand(2)*0\n",
    "        return u0.flatten()\n",
    "    return sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703200fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def generate_data(ntrajectories, t_sample,system,integrator, true_derivatives = False,H0=None,u0s=None,data_type = torch.float32):\n",
    "    #Parameters\n",
    "    nstates = system.nstates\n",
    "    traj_length = t_sample.shape[0] \n",
    "\n",
    "    #Initializing \n",
    "    u = np.zeros((ntrajectories,traj_length,nstates))\n",
    "    dudt = np.zeros_like(u)\n",
    "    t = np.zeros((ntrajectories,traj_length))\n",
    "\n",
    "    u0_ = np.zeros((ntrajectories,nstates))\n",
    "\n",
    "    for i in tqdm(range(ntrajectories)):\n",
    "        if u0s is not None:\n",
    "            u0 = np.array(u0s[i])\n",
    "            u[i], dudt[i], t[i],u0_[i] = system.sample_trajectory(t=t_sample,u0=u0,integrator=integrator)\n",
    "        else:\n",
    "            u[i], dudt[i], t[i],u0_[i] = system.sample_trajectory(t=t_sample,integrator=integrator)\n",
    "    \n",
    "    #Reshaping\n",
    "    dt = torch.tensor([t[0, 1] - t[0, 0]], dtype=data_type)\n",
    "    u_start = torch.tensor(u[:, :-1], dtype=data_type).reshape(-1, nstates)\n",
    "    u_end = torch.tensor(u[:, 1:], dtype=data_type).reshape(-1, nstates)\n",
    "    t_start = torch.tensor(t[:, :-1], dtype=data_type).reshape(-1, 1)\n",
    "    dt = dt * torch.ones_like(t_start, dtype=data_type)\n",
    "\n",
    "    if true_derivatives:\n",
    "        dudt = torch.tensor(dudt[:, :-1], dtype=data_type).reshape(-1, 1, nstates)\n",
    "    else:\n",
    "        dudt = (u_end - u_start).clone().detach() / dt[0, 0]\n",
    "\n",
    "    u_exact = u\n",
    "    return (u_start, u_end,t_start, dt), dudt, u_exact, u0_\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280a491",
   "metadata": {},
   "source": [
    "### Looking at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dabb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max_train = 18000\n",
    "alpha, beta, omega, gamma, delta = -1, 1, 1.4, 0.39, 0.1\n",
    "dt_per_period = 100\n",
    "nsamples_train = 18000/(2 * np.pi / omega / dt_per_period)\n",
    "ntraj_train = 1\n",
    "\n",
    "dt_train = T_max_train/nsamples_train\n",
    "nt_train = round(T_max_train / dt_train)\n",
    "t_train = np.linspace(0, T_max_train, nt_train + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d078e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "integrator = \"midpoint\"\n",
    "\n",
    "sys = DuffingSystem(alpha = alpha, beta = beta, omega = omega, delta = delta, gamma=gamma)\n",
    "\n",
    "(u_start, u_end, t_start, dt), dudt, u_train, u0s_train  =  generate_data(system=sys,ntrajectories =ntraj_train, t_sample = t_train,integrator=integrator)\n",
    "train_data = (u_start, u_end,t_start, dt), dudt\n",
    "\n",
    "pstep = 100\n",
    "plt.scatter(u_train[0][::pstep,0],u_train[0][::pstep,1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange\n",
    "import datetime\n",
    "\n",
    "\n",
    "def Batch_Data(data,batch_size,shuffle):\n",
    "    #Power of 2\n",
    " \n",
    "    nsamples = data[1].shape[0]\n",
    "\n",
    "    if shuffle:\n",
    "        permutation = torch.randperm(nsamples)\n",
    "    else:\n",
    "        permutation = torch.arange(nsamples)\n",
    "\n",
    "    nbatches = np.ceil(nsamples/batch_size).astype(int)\n",
    "    batched = [(None,None)] *nbatches  #((x_start, x_end, t_start, t_end, dt, u), dxdt)\n",
    "\n",
    "    for i in range(nbatches):\n",
    "        indices = permutation[i * batch_size : (i + 1) * batch_size]\n",
    "        input_tuple = [data[0][j][indices] for j in range(len(data[0]))]\n",
    "        dudt = data[1][indices]\n",
    "        batched[i] = (input_tuple, dudt)\n",
    "\n",
    "    return batched\n",
    "\n",
    "\n",
    "def penalty_loss(model,time, lam_F=1e-4,lam_N=1e-4):\n",
    "    penalty = 0\n",
    "    penalty += lam_F*torch.mean(torch.abs(model.External_Force(time.reshape(-1,1))))\n",
    "    penalty += lam_N*torch.mean(torch.abs(model.Get_N()))\n",
    "    return penalty\n",
    "\n",
    "def train_one_epoch(model,batched_train_data,loss_func,optimizer,integrator):\n",
    "    computed_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for input_tuple, dudt in batched_train_data:\n",
    "      \n",
    "        (u_start, u_end, t_start, dt) = input_tuple\n",
    "        n,m = u_start.shape\n",
    "        #Reshaping\n",
    "        if n ==1:\n",
    "            u_start = u_start.view(-1)\n",
    "\n",
    "        dudt = dudt.view(n,m)\n",
    "        #Estimating dudt\n",
    " \n",
    "\n",
    "        dudt_est = model.time_derivative_step(integrator = integrator, u_start = u_start,t_start = t_start,u_end = u_end,dt = dt)\n",
    "        loss = loss_func(dudt_est,dudt)\n",
    "        loss += penalty_loss(model,t_start)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        computed_loss += loss.item()\n",
    "\n",
    "    return computed_loss / len(batched_train_data)\n",
    "\n",
    "def compute_validation_loss(model, integrator, val_data, valdata_batched, loss_func):\n",
    "    val_loss = 0\n",
    "    if valdata_batched is not None:\n",
    "        for input_tuple, dudt in valdata_batched:\n",
    "            (u_start, u_end, t_start, dt) = input_tuple\n",
    "            n,m = u_start.shape\n",
    "            #Reshaping\n",
    "            if n ==1:\n",
    "                u_start = u_start.view(-1)\n",
    "            #u_start = u_start.requires_grad_()\n",
    "            dudt = dudt.view(n,m)\n",
    "        \n",
    "            dudt_est = model.time_derivative_step(integrator = integrator, u_start = u_start,t_start = t_start,u_end = u_end,dt = dt)\n",
    "\n",
    "            val_loss += loss_func(dudt_est, dudt).item()\n",
    "            val_loss += penalty_loss(model,t_start).item()\n",
    "    else:\n",
    "        (u_start, u_end, t_start, dt), dudt = val_data\n",
    "        n,m = u_start.shape\n",
    "        #Reshaping\n",
    "        if n ==1:\n",
    "            u_start = u_start.view(-1)\n",
    "        #u_start = u_start.requires_grad_()\n",
    "        dudt = dudt.view(n,m)\n",
    "        dudt_est = model.time_derivative(integrator, u_start,u_end,dt)\n",
    "        val_loss = loss_func(dudt_est, dudt).item()\n",
    "        val_loss += penalty_loss(model,t_start).item()\n",
    "    val_loss = val_loss / len(valdata_batched)\n",
    "    return val_loss#.item() #float(val_loss.detach().numpy())\n",
    "    \n",
    "\n",
    "def train(model,integrator, train_data,val_data, optimizer,shuffle,loss_func=torch.nn.MSELoss(),batch_size=1024,epochs = 20, verbose =True):\n",
    "\n",
    "   \n",
    "    trainingdetails={}\n",
    "    train_batch = Batch_Data(train_data, batch_size, shuffle)\n",
    "    valdata_batched = Batch_Data(val_data, batch_size, False)\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    loss_list = []\n",
    "    val_loss_list =  []\n",
    "\n",
    "\n",
    "\n",
    "    with trange(epochs) as steps:\n",
    "        for epoch in steps:\n",
    "            if shuffle:\n",
    "                train_batch = Batch_Data(train_data,batch_size,shuffle)\n",
    "            model.train(True) \n",
    "            start = datetime.datetime.now() \n",
    "            avg_loss = train_one_epoch(model,train_batch,loss_func,optimizer,integrator)\n",
    "            end = datetime.datetime.now() \n",
    "  \n",
    "            loss_list.append(avg_loss)\n",
    "            model.train(False) \n",
    "            if verbose: #Print\n",
    "                steps.set_postfix(epoch=epoch, loss=avg_loss)\n",
    "\n",
    "            if val_data is not None:\n",
    "                start = datetime.datetime.now()\n",
    "                vloss = compute_validation_loss(model, integrator, val_data, valdata_batched, loss_func)\n",
    "                end = datetime.datetime.now()\n",
    "               \n",
    "                val_loss_list.append(vloss)\n",
    "\n",
    "            trainingdetails[\"epochs\"] = epoch + 1\n",
    "            trainingdetails[\"val_loss\"] = vloss\n",
    "            trainingdetails[\"train_loss\"] = avg_loss\n",
    "\n",
    "\n",
    "\n",
    "    print(val_loss_list)\n",
    "    print(loss_list)\n",
    "     # Plot the loss curve\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(loss_list, label = \"Training Loss\")\n",
    "    plt.plot(val_loss_list,label = \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()\n",
    "\n",
    "    shape_data = (len(train_batch),train_batch[0][0][0].shape)\n",
    "\n",
    "    return model,trainingdetails\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590085de",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max_train = 2*np.pi\n",
    "chaos = False\n",
    "if chaos == True:\n",
    "    alpha, beta, omega, gamma, delta = 1, 1, 1.4, 0.39, 0.1 \n",
    "else:\n",
    "    alpha, beta, omega, gamma, delta = -1, 1, 1.2, 0.2, 0.3\n",
    "nsamples_train = 100*T_max_train/2\n",
    "ntraj_train =1750#700# 70\n",
    "T_max_val = T_max_train\n",
    "nsamples_val = nsamples_train\n",
    "ntraj_val = 750#300#30\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "\n",
    "act_1 = PAU()\n",
    "act_2 = PAU()\n",
    "act_3 = nn.Softplus()\n",
    "\n",
    "\n",
    "\n",
    "sys = DuffingSystem(alpha = alpha, beta = beta, omega = omega, delta = delta, gamma=gamma)\n",
    "\n",
    "dt_train = T_max_train/nsamples_train\n",
    "nt_train = round(T_max_train / dt_train)\n",
    "t_train = np.linspace(0, T_max_train, nt_train + 1)\n",
    "\n",
    "dt_val = T_max_val/nsamples_val\n",
    "nt_val = round(T_max_val / dt_val)\n",
    "t_val= np.linspace(0, T_max_val, nt_val + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Duffing_H_loss_func(dudt,dudt_est):\n",
    "    func = torch.nn.MSELoss()\n",
    "    MSE_dudt = func(dudt_est,dudt)\n",
    "    return MSE_dudt\n",
    "\n",
    "\n",
    "def Duffing_penalty_loss_func(model,time, lam_F=1e-4,lam_N=1e-4):\n",
    "    penalty = 0\n",
    "    penalty += lam_F*torch.mean(torch.abs(model.External_Force(time.reshape(-1,1)).detach()))\n",
    "    penalty += lam_N*torch.mean(torch.abs(model.Get_N().detach()))\n",
    "    return penalty\n",
    "\"\"\"\n",
    "def Duffing_penalty_loss_func(model, time, lam_F=1e-4, lam_N=1e-4):\n",
    "    with torch.no_grad(): \n",
    "        F = model.External_Force(time.reshape(-1, 1)).detach()\n",
    "        N = model.Get_N().detach()\n",
    "    return lam_F * torch.mean(torch.abs(F)) + lam_N * torch.mean(torch.abs(N))\n",
    "\"\"\"\n",
    "loss_func = loss_wrapper(Duffing_H_loss_func)\n",
    "penalty_func = loss_wrapper(Duffing_penalty_loss_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6833dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1750/1750 [00:13<00:00, 132.54it/s]\n",
      "100%|██████████| 750/750 [00:05<00:00, 135.68it/s]\n"
     ]
    }
   ],
   "source": [
    "integrator = \"midpoint\"\n",
    "\n",
    "(u_start, u_end, t_start, dt), dudt, u_train, u0s_train  =  generate_data(system=sys,ntrajectories =ntraj_train, t_sample = t_train,integrator=integrator)\n",
    "train_data = (u_start, u_end,t_start, dt), dudt\n",
    "\n",
    "(u_start, u_end,t_start, dt), dudt, u_val, u0s_val =  generate_data(system=sys,ntrajectories =ntraj_val, t_sample = t_val,integrator=integrator)\n",
    "val_data = (u_start, u_end, t_start, dt), dudt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a49a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hamiltonian_est = BaseHamiltonianNeuralNetwork(nstates = 2, act_1 = act_1, act_2 = act_2, act_3 = act_3)\n",
    "External_Forces_est = ExternalForceNeuralNetwork(nstates = 2, act_1 = act_1, act_2 = act_2, act_3 = act_3)\n",
    "\n",
    "\n",
    "model_exp = PortHamiltonianNeuralNetwork(nstates = 2, S = sys.S, Hamiltonian_est = Hamiltonian_est, External_Forces_est = External_Forces_est)\n",
    "\n",
    "optimizer_exp = torch.optim.Adam(model_exp.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "trainer = Training(model = model_exp,integrator = integrator, train_data = train_data, val_data = val_data,optimizer = optimizer_exp, system = sys, batch_size=batch_size,epochs = epochs)\n",
    "model_exp, trainingdetails_symp = trainer.train(loss_func=loss_func,penalty_func = penalty_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ae814",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = 10*np.pi\n",
    "nsamples = 100*tmax\n",
    "dt = tmax/nsamples\n",
    "\n",
    "nt = round(tmax / dt)\n",
    "t_sample = np.linspace(0, tmax, nt + 1)\n",
    "integrator = \"midpoint\"\n",
    "ntrajectories = 1\n",
    "\n",
    "(u_start, u_end, t_start, dt), dudt, u_exact, u0s =  generate_data(system=sys,ntrajectories =ntrajectories, t_sample = t_sample,integrator=integrator)\n",
    "\n",
    "u_phnn_exp, t_sample = model_exp.generate_trajectories(ntrajectories = ntrajectories, t_sample = t_sample,integrator = integrator,u0s=u0s)\n",
    "\n",
    "u_pred = u_phnn_exp[0].detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))  \n",
    "y, py = u_exact[0][:, 0], u_exact[0][:, 1]\n",
    "ax.plot(y, py, label=\"Exact\")\n",
    "y, py = u_pred[:, 0], u_pred[:, 1]\n",
    "ax.plot(y, py, label=\"PHNN\")\n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(\"py\")\n",
    "ax.set_title(\"Phase Space Trajectory Explicit Midpoint\" )\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3c344",
   "metadata": {},
   "source": [
    "## Symplectic Midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e64d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1750/1750 [01:17<00:00, 22.55it/s]\n",
      "100%|██████████| 750/750 [00:32<00:00, 23.09it/s]\n"
     ]
    }
   ],
   "source": [
    "integrator = \"symplectic midpoint\"\n",
    "\n",
    "(u_start, u_end, t_start, dt), dudt, u_train, u0s_train  =  generate_data(system=sys,ntrajectories =ntraj_train, t_sample = t_train,integrator=integrator, u0s = u0s_train)\n",
    "train_data = (u_start, u_end,t_start, dt), dudt\n",
    "\n",
    "(u_start, u_end,t_start, dt), dudt, u_val, u0s_val =  generate_data(system=sys,ntrajectories =ntraj_val, t_sample = t_val,integrator=integrator, u0s = u0s_val)\n",
    "val_data = (u_start, u_end, t_start, dt), dudt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfa317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [1:09:32<35:45, 126.23s/it, epoch=32, loss=4.95e-5]"
     ]
    }
   ],
   "source": [
    "Hamiltonian_est = BaseHamiltonianNeuralNetwork(nstates = 2, act_1 = act_1, act_2 = act_2, act_3 = act_3)\n",
    "External_Forces_est = ExternalForceNeuralNetwork(nstates = 2, act_1 = act_1, act_2 = act_2, act_3 = act_3)\n",
    "\n",
    "\n",
    "model_symp = PortHamiltonianNeuralNetwork(nstates = 2, S = sys.S, Hamiltonian_est = Hamiltonian_est, External_Forces_est = External_Forces_est)\n",
    "\n",
    "optimizer_symp = torch.optim.Adam(model_symp.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "trainer = Training(model = model_symp,integrator = integrator, train_data = train_data, val_data = val_data,optimizer = optimizer_symp, system = sys, batch_size=batch_size,epochs = epochs)\n",
    "model_symp, trainingdetails_symp = trainer.train(loss_func=loss_func,penalty_func = penalty_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09333b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = 10*np.pi\n",
    "nsamples = 100*tmax\n",
    "dt = tmax/nsamples\n",
    "\n",
    "nt = round(tmax / dt)\n",
    "t_sample = np.linspace(0, tmax, nt + 1)\n",
    "integrator = \"symplectic midpoint\"\n",
    "ntrajectories = 1\n",
    "\n",
    "(u_start, u_end, t_start, dt), dudt, u_exact, u0s =  generate_data(system=sys,ntrajectories =ntrajectories, t_sample = t_sample,integrator=integrator)\n",
    "\n",
    "u_phnn_symp, t_sample = model_symp.generate_trajectories(ntrajectories = ntrajectories, t_sample = t_sample,integrator = integrator,u0s=u0s)\n",
    "\n",
    "u_pred = u_phnn_symp[0].detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))  \n",
    "y, py = u_exact[0][:, 0], u_exact[0][:, 1]\n",
    "ax.plot(y, py, label=\"Exact\")\n",
    "y, py = u_pred[:, 0], u_pred[:, 1]\n",
    "ax.plot(y, py, label=\"PHNN\")\n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(\"py\")\n",
    "ax.set_title(\"Phase Space Trajectory Explicit Midpoint\" )\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
